@article{giannone2021economic,
  title={Economic predictions with big data: The illusion of sparsity},
  author={Giannone, Domenico and Lenza, Michele and Primiceri, Giorgio E},
  journal={Econometrica},
  volume={89},
  number={5},
  pages={2409--2437},
  year={2021},
  publisher={Wiley Online Library}
}

@article{chan2023bayesian,
  title={Bayesian state space models in macroeconometrics},
  author={Chan, Joshua CC and Strachan, Rodney W},
  journal={Journal of Economic Surveys},
  volume={37},
  number={1},
  pages={58--75},
  year={2023},
  publisher={Wiley Online Library}
}

@article{kohns2025arr2,
  title={The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions},
  author={Kohns, David and Kallioinen, Noa and McLatchie, Yann and Vehtari, Aki},
  journal={Bayesian Analysis},
  volume={1},
  number={1},
  pages={1--32},
  year={2025},
  publisher={International Society for Bayesian Analysis}
}




@article{chan2024large,
  title={Large order-invariant Bayesian VARs with stochastic volatility},
  author={Chan, Joshua CC and Koop, Gary and Yu, Xuewen},
  journal={Journal of Business \& Economic Statistics},
  volume={42},
  number={2},
  pages={825--837},
  year={2024},
  publisher={Taylor \& Francis}
}

@article{huerta1999priors,
  title={Priors and component structures in autoregressive time series models},
  author={Huerta, Gabriel and West, Mike},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={61},
  number={4},
  pages={881--899},
  year={1999},
  publisher={Wiley Online Library}
}

@article{koval2024bayesian,
  title={Bayesian Reconciliation of Return Predictability},
  author={Koval, Borys and Fr{\"u}hwirth-Schnatter, Sylvia and S{\"o}gner, Leopold},
  journal={Studies in Nonlinear Dynamics \& Econometrics},
  volume={28},
  number={2},
  pages={337--378},
  year={2024},
  publisher={De Gruyter}
}

@article{westTimeSeriesDecomposition,
  title = {Time Series Decomposition},
  author = {West, Mike},
  abstract = {A constructive result on time series decomposition is presented and illustrated. Developed through dynamic linear models, the decomposition is useful in analysis of an observed time series through inference about underlying, latent component series that may have physical interpretations. Particular special cases include state space autoregressive component models, in which the decomposition is useful for isolating latent, quasi-cyclical components, in particular. Brief summaries of analyses of some geological records related to climatic change illustrate the result.},
  langid = {english}
}


@book{pradoTimeSeriesModeling2021,
  title = {Time Series: Modeling, Computation, and Inference},
  shorttitle = {Time Series},
  author = {Prado, Raquel and Ferreira, Marco A. R. and West, Mike},
  year = {2021},
  series = {Texts in Statistical Science},
  edition = {Second edition},
  publisher = {CRC Press, Taylor \& Francis Group},
  address = {Boca Raton London New York},
  isbn = {978-1-4987-4702-8},
  langid = {english}
}


@article{potjagailo2023flexible,
  title={Flexible Bayesian MIDAS: time-variation, group-shrinkage and sparsity},
  author={Potjagailo, Galina and Kohns, David},
  year={2023},
  journal={Bank of England Working Paper},
  url={https://www.bankofengland.co.uk/working-paper/2023/flexible-bayesian-midas-time-variation-group-shrinkage-and-sparsity},
  publisher={Bank of England Working Paper},
  location = {London, UK}
}

@article{kovalBayesianReconciliationReturn2024,
  title = {Bayesian {{Reconciliation}} of {{Return Predictability}}},
  author = {Koval, Borys and {Fr{\"u}hwirth-Schnatter}, Sylvia and S{\"o}gner, Leopold},
  year = {2024},
  month = may,
  journal = {Studies in Nonlinear Dynamics \& Econometrics},
  volume = {28},
  number = {2},
  pages = {337--378},
  issn = {1558-3708},
  doi = {10.1515/snde-2022-0110},
  langid = {english}
}

@book{gelmanRegressionOtherStories2020,
  title = {Regression and {{Other Stories}}},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  year = {2020},
  publisher = {Cambridge University Press},
  location = {S.l.},
  isbn = {978-1-107-02398-7},
  langid = {english},
  annotation = {OCLC: 1150969622}
}


@article{fruhwirth2004efficient,
  title={Efficient Bayesian parameter estimation},
  author={Fr\"{u}hwirth-Schnatter, Sylvia},
  journal={State Space and Unobserved Component Models: Theory and Applications. Press Syndicate and the University of Cambridge},
  pages={123--151},
  year={2004}
}

@incollection{ghysels2020mixed,
  title={Mixed data sampling (MIDAS) regression models},
  author={Ghysels, Eric and Kvedaras, Virmantas and Zemlys-Balevi{\v{c}}ius, Vaidotas},
  booktitle={Handbook of statistics},
  volume={42},
  pages={117--153},
  year={2020},
  publisher={Elsevier}
}

@article{mogliani2021bayesian,
  title={Bayesian MIDAS penalized regressions: estimation, selection, and prediction},
  author={Mogliani, Matteo and Simoni, Anna},
  journal={Journal of Econometrics},
  volume={222},
  number={1},
  pages={833--860},
  year={2021},
  publisher={Elsevier}
}

@article{liseo2013objective,
  title={Objective priors for causal {AR(p)} with partial autocorrelations},
  author={Liseo, Brunero and Macaro, Christian},
  journal={Journal of Statistical Computation and Simulation},
  volume={83},
  number={9},
  pages={1613--1628},
  year={2013},
  publisher={Taylor \& Francis}
}

@article{berger1994noninformative,
  title={Noninformative priors and {Bayesian} testing for the {AR(1)} model},
  author={Berger, James O and Yang, Ruo-Yong},
  journal={Econometric Theory},
  volume={10},
  number={3-4},
  pages={461--482},
  year={1994},
  publisher={Cambridge University Press}
}

@book{zellner1996introduction,
  title={An introduction to Bayesian inference in econometrics},
  author={Zellner, Arnold},
  publisher={Wiley},
  year={1996}
}

@article{simpson2017penalising,
  title={Penalising model component complexity: A principled, practical approach to constructing priors},
  author={Simpson, Daniel and Rue, H{\aa}vard and Riebler, Andrea and Martins, Thiago G and S{\o}rbye, Sigrunn H},
  year={2017},
  journal={Statistical Science},
  pages={1--28},
  volume={32},
  number={1}
}

@article{polson2010shrink,
  title={Shrink globally, act locally: Sparse {Bayesian} regularization and prediction},
  author={Polson, Nicholas G and Scott, James G},
  journal={Bayesian Statistics},
  volume={9},
  number={501-538},
  pages={105},
  year={2010},
  publisher={Citeseer}
}

@book{west2006bayesian,
  title={Bayesian forecasting and dynamic models},
  author={West, Mike and Harrison, Jeff},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{brown2010inference,
  title={Inference with normal-gamma prior distributions in regression problems},
  author={Brown, Philip J and Griffin, Jim E},
  year={2010},
  journal={Bayesian Analysis},
  pages={171--188},
  volume={5},
  number={1}
}

@article{chang2022testing,
  title={Testing for unit roots based on sample autocovariances},
  author={Chang, Jinyuan and Cheng, Guanghui and Yao, Qiwei},
  journal={Biometrika},
  volume={109},
  number={2},
  pages={543--550},
  year={2022},
  publisher={Oxford University Press}
}

@article{aguilar2024generalized,
  title = {Generalized Decomposition Priors on {R2}},
  author = {Aguilar, Javier Enrique and B{\"u}rkner, Paul-Christian},
  journal = {arXiv preprint arXiv:2401.10180},
  year = {2024}
}

@article{burkner2017brms,
  title={{brms: An R package for Bayesian multilevel models using Stan}},
  author={B{\"u}rkner, Paul-Christian},
  journal={Journal of Statistical Software},
  volume={80},
  pages={1--28},
  year={2017}
}

@article{piironen2020projective,
  title={Projective inference in high-dimensional problems: Prediction and feature selection},
  author={Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki},
  year={2020},
  journal={Electronic Journal of Statistics},
  pages={2155--2197},
  volume={14},
  number={1}
}

@article{kohns2024horseshoe,
  title={Horseshoe prior {Bayesian} quantile regression},
  author={Kohns, David and Szendrei, Tibor},
  journal={Journal of the Royal Statistical Society Series C: Applied Statistics},
  volume={73},
  number={1},
  pages={193--220},
  year={2024},
  publisher={Oxford University Press US}
}

@book{johnson1995continuous,
  title={Continuous univariate distributions, volume 2},
  author={Johnson, Norman L and Kotz, Samuel and Balakrishnan, Narayanaswamy},
  volume={289},
  year={1995},
  publisher={John Wiley \& Sons}
}

@article{litterman1986forecasting,
  title={Forecasting with {Bayesian} vector autoregressions—five years of experience},
  author={Litterman, Robert B},
  journal={Journal of Business \& Economic Statistics},
  volume={4},
  number={1},
  pages={25--38},
  year={1986},
  publisher={Taylor \& Francis}
}

@article{bhattacharya2015dirichlet,
  title={{Dirichlet--Laplace} priors for optimal shrinkage},
  author={Bhattacharya, Anirban and Pati, Debdeep and Pillai, Natesh S and Dunson, David B},
  journal={Journal of the American Statistical Association},
  volume={110},
  number={512},
  pages={1479--1490},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{ishwaran2005spike,
  title={Spike and slab variable selection: frequentist and {Bayesian} strategies},
  author={Ishwaran, Hemant and Rao, J Sunil},
  year={2005},
  journal={Annals of Statistics},
  pages={730--773},
  volume={33},
  number={2}
}

@article{gelman2020bayesian,
  title={Bayesian workflow},
  author={Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"u}rkner, Paul-Christian and Modr{\'a}k, Martin},
  journal={arXiv preprint arXiv:2011.01808},
  year={2020}
}

@article{phillips1986understanding,
  title={Understanding spurious regressions in econometrics},
  author={Phillips, Peter CB},
  journal={Journal of econometrics},
  volume={33},
  number={3},
  pages={311--340},
  year={1986},
  publisher={Elsevier}
}

@article{doan1984forecasting,
  title={Forecasting and conditional projection using realistic prior distributions},
  author={Doan, Thomas and Litterman, Robert and Sims, Christopher},
  journal={Econometric reviews},
  volume={3},
  number={1},
  pages={1--100},
  year={1984},
  publisher={Taylor \& Francis}
}

@article{mclatchieRobustEfficientProjection2023,
  title = {Advances in Projection Predictive Inference},
  author = {McLatchie, Yann and R{\"o}gnvaldsson, S{\"o}lvi and Weber, Frank and Vehtari, Aki},
  year = {2025},
  journal = {Statistical Science},
volume = {40},
number = {1},
pages = {128-147},
  doi = {10.1214/24-STS949}
}


@article{kallioinenDetectingDiagnosingPrior2023,
author = {Kallioinen, Noa and Paananen, Topi and B\"{u}rkner, Paul-Christian and Vehtari, Aki},
journal = {Statistics and Computing},
year = {2024},
title = {Detecting and diagnosing prior and likelihood sensitivity with power-scaling},
doi = {10.1007/s11222-023-10366-5},
volume = {34},
pages = {57}
}


@article{piironenComparisonBayesianPredictive2017,
  title = {Comparison of {{Bayesian}} Predictive Methods for Model Selection},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  month = may,
  journal = {Statistics and Computing},
  volume = {27},
  number = {3},
  pages = {711--735},
  issn = {1573-1375},
  doi = {10.1007/s11222-016-9649-y},
  urldate = {2022-10-21},
  abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
  langid = {english},
  keywords = {Bayesian model selection,Cross-validation,Projection,Reference model,Selection bias}
}

@article{koop2011uk,
  title={UK macroeconomic forecasting with many predictors: Which models forecast best and when do they do so?},
  author={Koop, Gary and Korobilis, Dimitris},
  journal={Economic Modelling},
  volume={28},
  number={5},
  pages={2307--2318},
  year={2011},
  publisher={Elsevier}
}

@article{athanasopoulos2008varma,
  title={{VARMA} versus {VAR} for macroeconomic forecasting},
  author={Athanasopoulos, George and Vahid, Farshid},
  journal={Journal of Business \& Economic Statistics},
  volume={26},
  number={2},
  pages={237--252},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{mccracken2016fred,
  title={{FRED-MD}: A monthly database for macroeconomic research},
  author={McCracken, Michael W and Ng, Serena},
  journal={Journal of Business \& Economic Statistics},
  volume={34},
  number={4},
  pages={574--589},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{bitto2019achieving,
  title={Achieving shrinkage in a time-varying parameter model framework},
  author={Bitto, Angela and Fr{\"u}hwirth-Schnatter, Sylvia},
  journal={Journal of Econometrics},
  volume={210},
  number={1},
  pages={75--97},
  year={2019},
  publisher={Elsevier}
}

@article{huber2021inducing,
  title={Inducing sparsity and shrinkage in time-varying parameter models},
  author={Huber, Florian and Koop, Gary and Onorante, Luca},
  journal={Journal of Business \& Economic Statistics},
  volume={39},
  number={3},
  pages={669--683},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{allataifehSimultaneousPiezoelectricNoninvasive2020,
  title = {Simultaneous Piezoelectric Noninvasive Detection of Multiple Vital Signs},
  author = {Allataifeh, Areen and Al Ahmad, Mahmoud},
  year = {2020},
  month = jan,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {416},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-57326-6},
  urldate = {2023-11-07},
  abstract = {The monitoring of vital signs plays a key role in the diagnosis of several diseases. Piezoelectric sensors have been utilized to collect a corresponding representative signal from the chest surface. The subject typically needs to hold his or her breath to eliminate the respiration effect. This work further contributes to the extraction of the corresponding representative vital signs directly from the measured respiration signal. The contraction and expansion of the heart muscles, as well as the respiration activities, will induce a mechanical vibration across the chest wall. The induced vibration is then captured by the piezoelectric sensor placed at the chest surface, which produces an electrical output voltage signal conformally mapped with the respiration-cardiac activities. During breathing, the measured voltage signal is composed of the cardiac cycle activities modulated along with the respiratory cycle activity. A representative model that incorporates the cardiac and respiratory activities is developed and adopted. The piezoelectric and the convolution theories along with Fourier transformation are applied to extract the corresponding cardiac activity signal from the respiration signal. All the results were validated step by step by a conventional apparatus, with good agreement observed.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Characterization and analytical techniques,Electrical and electronic engineering},
  file = {/u/83/kallion6/unix/Zotero/storage/FNWGF73S/Allataifeh and Al Ahmad - 2020 - Simultaneous piezoelectric noninvasive detection o.pdf}
}

@article{barraza-barrazaAdaptiveARXModel2017,
  title = {An Adaptive {{ARX}} Model to Estimate the {{RUL}} of Aluminum Plates Based on Its Crack Growth},
  author = {{Barraza-Barraza}, Diana and {Tercero-G{\'o}mez}, V{\'i}ctor G. and Beruvides, Mario G. and {Lim{\'o}n-Robles}, Jorge},
  year = {2017},
  month = jan,
  journal = {Mechanical Systems and Signal Processing},
  volume = {82},
  pages = {519--536},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2016.05.041},
  urldate = {2023-11-07},
  abstract = {A wide variety of Condition-Based Maintenance (CBM) techniques deal with the problem of predicting the time for an asset fault. Most statistical approaches rely on historical failure data that might not be available in several practical situations. To address this issue, practitioners might require the use of self-starting approaches that consider only the available knowledge about the current degradation process and the asset operating context to update the prognostic model. Some authors use Autoregressive (AR) models for this purpose that are adequate when the asset operating context is constant, however, if it is variable, the accuracy of the models can be affected. In this paper, three autoregressive models with exogenous variables (ARX) were constructed, and their capability to estimate the remaining useful life (RUL) of a process was evaluated following the case of the aluminum crack growth problem. An existing stochastic model of aluminum crack growth was implemented and used to assess RUL estimation performance of the proposed ARX models through extensive Monte Carlo simulations. Point and interval estimations were made based only on individual history, behavior, operating conditions and failure thresholds. Both analytic and bootstrapping techniques were used in the estimation process. Finally, by including recursive parameter estimation and a forgetting factor, the ARX methodology adapts to changing operating conditions and maintain the focus on the current degradation level of an asset.},
  keywords = {Aluminum plates,ARX,Bootstrap simulation,Condition-Based Maintenance,Prognostics,Recursive Least Squares,Remaining useful life},
  file = {/u/83/kallion6/unix/Zotero/storage/NAKM5G8B/Barraza-Barraza et al. - 2017 - An adaptive ARX model to estimate the RUL of alumi.pdf;/u/83/kallion6/unix/Zotero/storage/9C8A8G2T/S0888327016301327.html}
}

@misc{cooperCrossvalidatoryModelSelection2023,
  title = {Cross-Validatory Model Selection for {{Bayesian}} Autoregressions with Exogenous Regressors},
  author = {Cooper, Alex and Simpson, Dan and Kennedy, Lauren and Forbes, Catherine and Vehtari, Aki},
  year = {2023},
  month = mar,
  number = {arXiv:2301.08276},
  eprint = {2301.08276},
  primaryclass = {stat},
  publisher = {{arXiv}},
  urldate = {2023-08-31},
  abstract = {Bayesian cross-validation (CV) is a popular method for predictive model assessment that is simple to implement and broadly applicable. A wide range of CV schemes is available for time series applications, including generic leave-one-out (LOO) and K-fold methods, as well as specialized approaches intended to deal with serial dependence such as leave-future-out (LFO), h-block, and hv-block. Existing large-sample results show that both specialized and generic methods are applicable to models of serially-dependent data. However, large sample consistency results overlook the impact of sampling variability on accuracy in finite samples. Moreover, the accuracy of a CV scheme depends on many aspects of the procedure. We show that poor design choices can lead to elevated rates of adverse selection. In this paper, we consider the problem of identifying the regression component of an important class of models of data with serial dependence, autoregressions of order p with q exogenous regressors (ARX(p,q)), under the logarithmic scoring rule. We show that when serial dependence is present, scores computed using the joint (multivariate) density have lower variance and better model selection accuracy than the popular pointwise estimator. In addition, we present a detailed case study of the special case of ARX models with fixed autoregressive structure and variance. For this class, we derive the finite-sample distribution of the CV estimators and the model selection statistic. We conclude with recommendations for practitioners.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/u/83/kallion6/unix/Zotero/storage/UFWNWUXC/Cooper et al. - 2023 - Cross-validatory model selection for Bayesian auto.pdf;/u/83/kallion6/unix/Zotero/storage/RJ8EQWCW/2301.html}
}

@article{dahlinHierarchicalBayesianARX2012,
  title = {Hierarchical {{Bayesian ARX}} Models for Robust Inference},
  author = {Dahlin, Johan and Lindsten, Fredrik and Sch{\"o}n, Thomas B. and Wills, Adrian},
  year = {2012},
  month = jul,
  journal = {IFAC Proceedings Volumes},
  series = {16th {{IFAC Symposium}} on {{System Identification}}},
  volume = {45},
  number = {16},
  pages = {131--136},
  issn = {1474-6670},
  doi = {10.3182/20120711-3-BE-2027.00318},
  urldate = {2023-09-26},
  abstract = {Gaussian innovations are the typical choice in most ARX models but using other distributions such as the Student's t could be useful. We demonstrate that this choice of distribution for the innovations provides an increased robustness to data anomalies, such as outliers and missing observations. We consider these models in a Bayesian setting and perform inference using numerical procedures based on Markov Chain Monte Carlo methods. These models include automatic order determination by two alternative methods, based on a parametric model order and a sparseness prior, respectively. The methods and the advantage of our choice of innovations are illustrated in three numerical studies using both simulated data and real EEG data.},
  keywords = {ARX models,Bayesian methods,Markov chain Monte Carlo,Robust estimation},
  file = {/u/83/kallion6/unix/Zotero/storage/YVHG8ZI3/Dahlin et al. - 2012 - Hierarchical Bayesian ARX models for robust infere.pdf;/u/83/kallion6/unix/Zotero/storage/2HRX9Z9M/S1474667015379404.html}
}

@article{dahlinSparseBayesianARX2018,
  title = {Sparse {{Bayesian ARX}} Models with Flexible Noise Distributions},
  author = {Dahlin, Johan and Wills, Adrian and Ninness, Brett},
  year = {2018},
  month = jan,
  journal = {IFAC-PapersOnLine},
  series = {18th {{IFAC Symposium}} on {{System Identification SYSID}} 2018},
  volume = {51},
  number = {15},
  pages = {25--30},
  issn = {2405-8963},
  doi = {10.1016/j.ifacol.2018.09.085},
  urldate = {2023-09-26},
  abstract = {This paper considers the problem of estimating linear dynamic system models when the observations are corrupted by random disturbances with nonstandard distributions. The paper is particularly motivated by applications where sensor imperfections involve significant contribution of outliers or wrap-around issues resulting in multi-modal distributions such as commonly encountered in robotics applications. As will be illustrated, these nonstandard measurement errors can dramatically compromise the effectiveness of standard estimation methods, while a computational Bayesian approach developed here is demonstrated to be equally effective as standard methods in standard measurement noise scenarios, but dramatically more effective in nonstandard measurement noise distribution scenarios.},
  keywords = {Bayesian inference,Gaussian mixture models,Hamiltonian Monte Carlo},
  file = {/u/83/kallion6/unix/Zotero/storage/7R3G9TQ3/Dahlin et al. - 2018 - Sparse Bayesian ARX models with flexible noise dis.pdf;/u/83/kallion6/unix/Zotero/storage/B274BU2Z/S2405896318317452.html}
}

@inproceedings{fangBayesianInferenceFederated2021,
  title = {Bayesian {{Inference Federated Learning}} for {{Heart Rate Prediction}}},
  booktitle = {Wireless {{Mobile Communication}} and {{Healthcare}}},
  author = {Fang, Lei and Liu, Xiaoli and Su, Xiang and Ye, Juan and Dobson, Simon and Hui, Pan and Tarkoma, Sasu},
  editor = {Ye, Juan and O'Grady, Michael J. and Civitarese, Gabriele and Yordanova, Kristina},
  year = {2021},
  series = {Lecture {{Notes}} of the {{Institute}} for {{Computer Sciences}}, {{Social Informatics}} and {{Telecommunications Engineering}}},
  pages = {116--130},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-70569-5_8},
  abstract = {The advances of sensing and computing technologies pave the way to develop novel applications and services for wearable devices. For example, wearable devices measure heart rate, which accurately reflects the intensity of physical exercise. Therefore, heart rate prediction from wearable devices benefits users with optimization of the training process. Conventionally, Cloud collects user data from wearable devices and conducts inference. However, this paradigm introduces significant privacy concerns. Federated learning is an emerging paradigm that enhances user privacy by remaining the majority of personal data on users' devices. In this paper, we propose a statistically sound, Bayesian inference federated learning for heart rate prediction with autoregression with exogenous variable (ARX) model. The proposed privacy-preserving method achieves accurate and robust heart rate prediction. To validate our method, we conduct extensive experiments with real-world outdoor running exercise data collected from wearable devices.},
  isbn = {978-3-030-70569-5},
  langid = {english},
  keywords = {Bayesian inference,Federated learning,Heart rate prediction,Wearable computing},
  file = {/u/83/kallion6/unix/Zotero/storage/RPNQ9RS8/Fang et al. - 2021 - Bayesian Inference Federated Learning for Heart Ra.pdf}
}

@article{kaufmannEvidenceHumanInfluence1997,
  title = {Evidence for Human Influence on Climate from Hemispheric Temperature Relations},
  author = {Kaufmann, Robert K. and Stern, David I.},
  year = {1997},
  month = jul,
  journal = {Nature},
  volume = {388},
  number = {6637},
  pages = {39--44},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/40332},
  urldate = {2023-09-26},
  abstract = {Analysis of observational temperature records for the Northern and Southern hemispheres indicates a statistical relationship in which Northern Hemisphere temperature depends on temperature in the Southern Hemisphere. This pattern, which has strengthened over time, can be explained by the climatic effects of anthropogenic trace gases and tropospheric sulphate aerosols. A similar statistical patternis produced by model simulations of the historical atmosphere.},
  copyright = {1997 Macmillan Magazines Ltd.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/u/83/kallion6/unix/Zotero/storage/WXN5MQUF/Kaufmann and Stern - 1997 - Evidence for human influence on climate from hemis.pdf}
}

@article{khatibisepehrBayesianApproachRobust2013,
  title = {A {{Bayesian}} Approach to Robust Process Identification with {{ARX}} Models},
  author = {Khatibisepehr, Shima and Huang, Biao},
  year = {2013},
  journal = {AIChE Journal},
  volume = {59},
  number = {3},
  pages = {845--859},
  issn = {1547-5905},
  doi = {10.1002/aic.13887},
  urldate = {2023-09-26},
  abstract = {In the context of process industries, outlying observations mostly represent a large random error resulting from irregular process disturbances, instrument failures, or transmission problems. Statistical analysis of process data contaminated with outliers may lead to biased parameter estimation and plant-model mismatch. The problem of process identification in the presence of outliers has received great attention and a wide variety of outlier identification approaches have been proposed. However, there is a great need to seek for more general solutions and a robust framework to deal with different types of outliers. The main objective of this work is to formulate and solve the robust process identification problem under a Bayesian framework. The proposed solution strategy not only yields maximum a posteriori estimates of model parameters but also provides hyperparameters that determine data quality as well as prior distribution of model parameters. Identification of a simulated continuous fermentation reactor is considered to show the effectiveness and robustness of the proposed Bayesian framework. The advantages of the method are further illustrated through an experimental case study of a pilot-scale continuous stirred tank heater. \textcopyright{} 2012 American Institute of Chemical Engineers AIChE J, 59: 845\textendash 859, 2013},
  copyright = {Copyright \textcopyright{} 2012 American Institute of Chemical Engineers (AIChE)},
  langid = {english},
  keywords = {Bayesian inference,outliers,process identification},
  file = {/u/83/kallion6/unix/Zotero/storage/ISW5ARQW/Khatibisepehr and Huang - 2013 - A Bayesian approach to robust process identificati.pdf;/u/83/kallion6/unix/Zotero/storage/5TSESCD4/aic.html}
}

@article{kimLongtermBridgeHealth2018,
  title = {Long-Term Bridge Health Monitoring and Performance Assessment Based on a {{Bayesian}} Approach},
  author = {Kim, Chul-Woo and Zhang, Yi and Wang, Ziran and Oshima, Yoshinobu and Morita, Tomoaki},
  year = {2018},
  month = jul,
  journal = {Structure and Infrastructure Engineering},
  volume = {14},
  number = {7},
  pages = {883--894},
  publisher = {{Taylor \& Francis}},
  issn = {1573-2479},
  doi = {10.1080/15732479.2018.1436572},
  urldate = {2023-09-26},
  abstract = {This study presents a damage detection approach for the long-term health monitoring of bridge structures. The Bayesian approach comprising both Bayesian regression and Bayesian hypothesis testing is proposed to detect the structural changes in an in-service seven-span steel plate girder bridge with Gerber system. Both temperature and vehicle weight effects are accounted in the analysis. The acceleration responses at four points of the bridge span are utilised in this investigation. The data covering three different time periods are used in the bridge health monitoring (BHM). Regression analyses showed that the autoregressive exogenous model considering both temperature and vehicle weight effects has the best performance. The Bayesian factor is found to be a sensitive damage indicator in the BHM. The Bayesian approach can provide updated information in the real-time monitoring of bridge structures. The information provided from the Bayesian approach is convenient and easy to handle compared to the traditional approaches. The applicability of this approach is also validated in a case study where artificially generated damage data is added to the observation data.},
  keywords = {Autoregressive model,Bayesian statistics,bridge health monitoring,damage detection,Kalman filter,long-term assessment,real bridge},
  file = {/u/83/kallion6/unix/Zotero/storage/88JGAJI6/Kim et al. - 2018 - Long-term bridge health monitoring and performance.pdf}
}

@article{matsuokaBayesianEstimationInstantaneous2021,
  title = {Bayesian Estimation of Instantaneous Frequency Reduction on Cracked Concrete Railway Bridges under High-Speed Train Passage},
  author = {Matsuoka, Kodai and Tokunaga, Munemasa and Kaito, Kiyoyuki},
  year = {2021},
  month = dec,
  journal = {Mechanical Systems and Signal Processing},
  volume = {161},
  pages = {107944},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2021.107944},
  urldate = {2023-09-26},
  abstract = {When a train passes along prestressed concrete (PC) bridges, crack opening reduces the girder stiffness, providing important indicators for the practical evaluation of bridge performance. However, this phenomenon is difficult to detect in the free vibrations induced by trains or via hammer tests as these vibrations have very small amplitudes. Moreover, although the bridge under a passing train vibrates with large amplitude, the vibrational state is forced, and the girder stiffness induced by crack opening occurs only in the downward displacement. To estimate this complex but valuable system, this study establishes a time-varying autoregressive with eXogenous (TV-ARX) model and a hierarchical Bayesian estimation (Bayesian TV-ARX) method. The exogenous variables in the TV-ARX model are the modal-transformed moving loads, which constitute the main excitation frequency of the running train. The variation of the autoregressive (AR) coefficient includes the modal characteristic as a random walk process. When a train passes, the Bayesian TV-ARX estimates the instantaneous amplitude-dependent drop of the bridge frequency from the displacement response. After formulating the Bayesian TV-ARX, the effects of the vehicle\textendash bridge interaction (VBI), train speed, and track irregularity on the accuracy of the instantaneous stiffness decrease estimated via the Bayesian TV-ARX were verified using VBI simulations of a nonlinear beam, in which the bending stiffness decreased only during a downward displacement. Even when the VBI effects overlapped, the model accurately estimated the reduced bridge stiffness due to crack opening. Next, the Bayesian TV-ARX was applied to two PC bridges on a real high-speed railway. On one of the bridges, the estimated stiffness decreased by~\textasciitilde ~12\% when the bridge was downward-displaced by crack opening during the train passage. Such results on a real bridge under operation have not been previously reported. Four months later, the amplitude-dependent decrease in the bridge frequency was amplified by crack propagation. These results are important evidences of a nonlinear and nonstationary system. Besides solving real problems, the proposed method is expected to significantly contribute to condition-based maintenance and structure-health monitoring of PC bridges. In particular, it enables early detection of damage and deterioration in addition to bridge performance evaluation over time.},
  keywords = {Bayesian TV-ARX,Breathing crack,Concrete bridge,High-speed railway,Time\textendash frequency analysis},
  file = {/u/83/kallion6/unix/Zotero/storage/QH2QKWNY/Matsuoka et al. - 2021 - Bayesian estimation of instantaneous frequency red.pdf}
}

@article{matsuokaBayesianTimeFrequency2020,
  title = {Bayesian Time\textendash Frequency Analysis of the Vehicle\textendash Bridge Dynamic Interaction Effect on Simple-Supported Resonant Railway Bridges},
  author = {Matsuoka, Kodai and Kaito, Kiyoyuki and Sogabe, Masamichi},
  year = {2020},
  month = jan,
  journal = {Mechanical Systems and Signal Processing},
  volume = {135},
  pages = {106373},
  issn = {0888-3270},
  doi = {10.1016/j.ymssp.2019.106373},
  urldate = {2023-09-26},
  abstract = {Monitoring the conditions or damages of bridges under train passages demands a high-accuracy modal-characteristic identification method that separates the apparent fluctuations caused by vehicle\textendash bridge dynamic interaction (VBI) effects from other fluctuations. This study proposes a novel method based on a time-varying autoregressive model, which is solved using a hierarchical Bayesian estimation approach. The VBI effect is estimated from the displacement response of the railway bridges as temporal fluctuations of the natural frequency and modal damping ratio. The exogenous variable is the train load, expressed as an external force. Numerical experiments verified the higher accuracy of the proposed method than the existing method. The influences of train speed and rail irregularity on the VBI effects are clarified by the application of the proposed method to various VBI simulations. The proposed method was applied to the measured resonance responses of actual bridges and succeeded in empirically demonstrating the decreased natural frequency and the increased modal damping ratio under train passage. Additionally, using the proposed method, modal characteristics variation due to VBI effect calculated using VBI model simulation was verified by comparing with those estimated from the measured results.},
  keywords = {Bayesian estimation,MCMC,Resonance,TVARX,Vehicle\textendash bridge interaction},
  file = {/u/83/kallion6/unix/Zotero/storage/EF3QSR8V/Matsuoka et al. - 2020 - Bayesian time–frequency analysis of the vehicle–br.pdf;/u/83/kallion6/unix/Zotero/storage/W4QGABQA/S0888327019305941.html}
}

@article{mustafarajPredictionRoomTemperature2011,
  title = {Prediction of Room Temperature and Relative Humidity by Autoregressive Linear and Nonlinear Neural Network Models for an Open Office},
  author = {Mustafaraj, G. and Lowry, G. and Chen, J.},
  year = {2011},
  month = jun,
  journal = {Energy and Buildings},
  volume = {43},
  number = {6},
  pages = {1452--1460},
  issn = {0378-7788},
  doi = {10.1016/j.enbuild.2011.02.007},
  urldate = {2023-11-07},
  abstract = {In this study, a linear parametric autoregressive model with external inputs (ARX) and a neural network-based nonlinear autoregressive model with external inputs (NNARX) are developed to predict the thermal behaviour of an open office in a modern building. External and internal climate data recorded over three months were used to build and validate models for predicting dry bulb temperature and relative humidity for different time-scales (30min to 3h ahead). In order to compare the accuracy for different step-ahead predictions, different performance measures, such as goodness of fit, mean squared error, mean absolute error and coefficient of determination between predicted model output and real measurements, were calculated. For the NNARX model, the optimal network structure after training, is subsequently determined by pruning the fully connected network using the optimal brain surgeon strategy. The results demonstrate that both models provide reasonably good predictions but the nonlinear NNARX model outperforms the linear ARX model. These models can both potentially be used for improving indoor air quality by focusing on building intelligence into the controller in HVAC plants, in particular, adaptive control systems.},
  keywords = {Black-box autoregressive linear and nonlinear neural network models,Building management system,Optimal brain surgeon pruning algorithm,Room temperature and relative humidity prediction},
  file = {/u/83/kallion6/unix/Zotero/storage/SJWTWMVH/Mustafaraj et al. - 2011 - Prediction of room temperature and relative humidi.pdf;/u/83/kallion6/unix/Zotero/storage/7V4MC5KF/S037877881100051X.html}
}

@inproceedings{nunesARXModelingDrug2013,
  title = {{{ARX}} Modeling of Drug Effects on Brain Signals during General Anesthesia},
  booktitle = {21st {{Mediterranean Conference}} on {{Control}} and {{Automation}}},
  author = {Nunes, Catarina S. and Lobo, Francisco A. and Amorim, Pedro and {de Anestesiologia}, Servi{\c c}o and {do Porto}, Centro Hospitalar},
  year = {2013},
  month = jun,
  pages = {202--205},
  doi = {10.1109/MED.2013.6608722},
  urldate = {2023-11-07},
  abstract = {The effect of drugs' interaction on the brain signal Bispectral Index (BIS) of the EEG, is of great importance for an anesthesia control drug infusion system. In this study, the objective was to investigate if an autoregressive with exogenous inputs model (ARX) could be a suitable approach to predicting BIS according to the anesthetic drugs concentrations. Data were collected in 45 neurosurgeries with total intravenous anesthesia every 5s. A stochastic ARX model was fitted to the data of each patient. The models structure that performed better as predictor used a 30s lag for BIS, 1min lag for propofol and 2min lag for remifentanil. The models had a good performance with statistical zero errors (P {$<$} 0.05) in 31 patients. The average of absolute errors was 8.2 {$\pm$} 2.5, showing that the model captures the brain signal trend. This model proved to be effective in modeling and one step prediction of the BIS signal capturing unique characteristics. The results show that the previous brain response trend has influence on the present value, in addition the drugs concentrations from the previous 2min still have influence. This is an important conclusion for the development of drug infusion controller algorithms.},
  file = {/u/83/kallion6/unix/Zotero/storage/HI37PHJ2/Nunes et al. - 2013 - ARX modeling of drug effects on brain signals duri.pdf;/u/83/kallion6/unix/Zotero/storage/M8RF3M4Y/6608722.html}
}

@inproceedings{polasekGibbsSamplingARX1996,
  title = {Gibbs {{Sampling}} in {{ARX Models}} with the ``{{Minnesota}}'' {{Prior}}},
  booktitle = {Operations {{Research Proceedings}} 1995},
  author = {Polasek, Wolfgang and Jin, Song},
  editor = {Kleinschmidt, Peter and Bachem, Achim and Derigs, Ulrich and Fischer, Dietrich and {Leopold-Wildburger}, Ulrike and M{\"o}hring, Rolf},
  year = {1996},
  series = {Operations {{Research Proceedings}}},
  pages = {223--228},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-80117-4_39},
  abstract = {In 1980ies Bayesian ARX models became an important instrument in economic forecasting (see e.g. Litterman 1986). This so-called Minnesota or Litterman prior consists of a hierarchical tightness structure, where 2 hyperparameters have to be specified by the user: {$\lambda$}, which controls the tightness of the own past of a time series and \texttheta{} which controls the tightness of the past of all other time series. The ARX model for the multivariate time series \{(y1t,\ldots,ykt), t = 1,\ldots,T\} explains one time series by the own history and the present and the past of the other time series. The ARX model we are analyzing consists of the following equations for a T x 1 vector y1: 1\$\$ \{y\_1\} = \{X\_1\}\{b\_1\} + \{X\_2\}\{b\_2\} + \textbackslash ldots + \{X\_K\}\{b\_K\} + \textbackslash varepsilon = XvecB + \textbackslash varepsilon \$\$with the T x p' K regressor matrix X = [X1: X2:\ldots : XK], where p' = p + 1. The first T x p' block X1 = [1n: y1,-1:\ldots : y1,-p] consists of the constant and the past of the left hand variable. Each Tx(p+1) block Xkrepresents the current and the past of the time series yk = \{ykt\}t=1,\ldots,T. Thus, the multiple tightness model with normal errors can be formulated as a linear regression model of the standard form y \textasciitilde{} N(Xb,{$\sigma$}2IT), and b = vecB = (b1:\ldots : bK) is the stacked coefficient vector for K time series responses, where B is a p' x K coefficient matrix. Note that all the K columns of B are b'k= (b0k, b1k,\ldots, bpk), k = 1,\ldots, K. The first row of B, i.e. {$\alpha$}' = (b01, b02,\ldots, b0K) contains the constant b01, and all other contemporaneous coefficients starting with the response of y2 on y1.},
  isbn = {978-3-642-80117-4},
  langid = {english}
}

@article{rios-morenoModellingTemperatureIntelligent2007,
  title = {Modelling Temperature in Intelligent Buildings by Means of Autoregressive Models},
  author = {{R{\'i}os-Moreno}, G. J. and {Trejo-Perea}, M. and {Casta{\~n}eda-Miranda}, R. and {Hern{\'a}ndez-Guzm{\'a}n}, V. M. and {Herrera-Ruiz}, G.},
  year = {2007},
  month = aug,
  journal = {Automation in Construction},
  volume = {16},
  number = {5},
  pages = {713--722},
  issn = {0926-5805},
  doi = {10.1016/j.autcon.2006.11.003},
  urldate = {2023-11-07},
  abstract = {One of the main problems of the intelligent buildings is to give comfort to its occupants and to increase the user's performance at a low cost. The excessive demand of electric energy due to heating, ventilating, and air-conditioning (HVAC) systems require temperature forecast and control to make maximum reduction of the electrical energy. The objective of this paper is to investigate in what extent linear autoregressive models with external input (ARX) and autoregressive moving average models with external input (ARMAX), could be used in order to predict the interior air temperature of a building. In particular, the obtained results in the classrooms of the Universidad Aut\'onoma de Quer\'etaro, U.A.Q., M\'exico, are shown. Outside air temperature, global solar radiation flux, outside air relative humidity and air velocity were used as the input variables. The obtained results showed that the ARX models give a better prediction of the temperature than the ARMAX models, obtaining the best results with the ARX (2,3,0) with a coefficient of determination of 0.9457 and ARX (2,2,1) with a coefficient of determination of 0.9056.},
  keywords = {Autoregressive models,Comfort,Intelligent buildings}
}


@article{sarwarFieldValidationStudy2017,
  title = {Field Validation Study of a Time and Temperature Indexed Autoregressive with Exogenous ({{ARX}}) Model for Building Thermal Load Prediction},
  author = {Sarwar, Riasat and Cho, Heejin and Cox, Sam J. and Mago, Pedro J. and Luck, Rogelio},
  year = {2017},
  month = jan,
  journal = {Energy},
  volume = {119},
  pages = {483--496},
  issn = {0360-5442},
  doi = {10.1016/j.energy.2016.12.083},
  urldate = {2023-11-07},
  abstract = {Building load prediction algorithms are becoming an essential component of building energy technologies as intelligent building technologies are rapidly evolving and require accurate load predictions to make real-time operational decisions. This paper presents a field validation study of an autoregressive with exogenous (ARX) model, indexed with respect to time and temperature, and used for hourly building thermal load prediction with an aim for integration with real time predictive control strategies. Indexing of the ARX model implies that different sets of coefficients are used in the predictive equation depending on different time intervals and temperature ranges. Although many regressive prediction models have been proposed, no field validation has been reported in the literature, which is an essential step before implementation in actual practice. The validation study was carried out using field data from three buildings located in the main campus of Mississippi State University. The proposed model was able to predict hourly thermal load accurately and within the uncertainty bounds of the measured thermal load most of the time. Results also demonstrated that proper indexing of the model allowed it to capture different cooling and heating load profiles and abrupt changes in the load pattern.},
  keywords = {Autoregressive with exogenous (ARX) model,Field measurements,Thermal load prediction,Uncertainty analysis,Validation}
}

@article{wangTrialtotrialNoiseCancellation2011,
  title = {Trial-to-Trial Noise Cancellation of Cortical Field Potentials in Awake Macaques by Autoregression Model with Exogenous Input ({{ARX}})},
  author = {Wang, Zheng and Roe, Anna W.},
  year = {2011},
  month = jan,
  journal = {Journal of Neuroscience Methods},
  volume = {194},
  number = {2},
  pages = {266--273},
  issn = {0165-0270},
  doi = {10.1016/j.jneumeth.2010.10.029},
  urldate = {2023-11-07},
  abstract = {Gamma band synchronization has drawn increasing interest with respect to its potential role in neuronal encoding strategy and behavior in awake, behaving animals. However, contamination of these recordings by power line noise can confound the analysis and interpretation of cortical local field potential (LFP). Existing denoising methods are plagued by inadequate noise reduction, inaccuracies, and even introduction of new noise components. To carefully and more completely remove such contamination, we propose an automatic method based on the concept of adaptive noise cancellation that utilizes the correlative features of common noise sources, and implement with AutoRegressive model with eXogenous Input (ARX). We apply this technique to both simulated data and LFPs recorded in the primary visual cortex of awake macaque monkeys. The analyses here demonstrate a greater degree of accurate noise removal than conventional notch filters. Our method leaves desired signal intact and does not introduce artificial noise components. Application of this method to awake monkey V1 recordings reveals a significant power increase in the gamma range evoked by visual stimulation. Our findings suggest that the ARX denoising procedure will be an important pre-processing step in the analysis of large volumes of cortical LFP data as well as high frequency (gamma-band related) electroencephalography/magnetoencephalography (EEG/MEG) applications, one which will help to convincingly dissociate this notorious artifact from gamma-band activity.},
  keywords = {Adaptive noise cancellation (ANC),Autoregression model with exogenous input (ARX),Awake macaque,Gamma band,Local field potential,Wavelet transform}
}

@article{yunBuildingHourlyThermal2012,
  title = {Building Hourly Thermal Load Prediction Using an Indexed {{ARX}} Model},
  author = {Yun, Kyungtae and Luck, Rogelio and Mago, Pedro J. and Cho, Heejin},
  year = {2012},
  month = nov,
  journal = {Energy and Buildings},
  volume = {54},
  pages = {225--233},
  issn = {0378-7788},
  doi = {10.1016/j.enbuild.2012.08.007},
  urldate = {2023-11-07},
  abstract = {This paper introduces an easily implementable and computationally efficient, ARX (autoregressive with exogenous, i.e., external, inputs) time and temperature indexed model for 1h ahead building thermal load prediction. Time and temperature indexing implies that different sets of coefficients are used in the predictive equation depending on the time of the day or the ambient temperature. The indexing and proposed structure of the model follows physically motivated interpretations of the loading conditions and thermal response of the building. One of the main contributions of the proposed model is that it allows determining the dominant factors that affect the thermal load at a given time. A free and widely adopted building energy and thermal load simulation program from the U.S. Department of Energy is used to determine the prediction accuracy of the proposed model on several different benchmark-building types: a small office building, a medium office building, a midrise apartment, and a high-rise apartment.},
  keywords = {Autoregressive model,Building thermal load prediction,Linear regression model}
}

@article{zanottiChoosingLinearNonlinear2019,
  title = {Choosing between Linear and Nonlinear Models and Avoiding Overfitting for Short and Long Term Groundwater Level Forecasting in a Linear System},
  author = {Zanotti, Chiara and Rotiroti, Marco and Sterlacchini, Simone and Cappellini, Giacomo and Fumagalli, Letizia and Stefania, Gennaro A. and Nannucci, Marco S. and Leoni, Barbara and Bonomi, Tullia},
  year = {2019},
  month = nov,
  journal = {Journal of Hydrology},
  volume = {578},
  pages = {124015},
  issn = {0022-1694},
  doi = {10.1016/j.jhydrol.2019.124015},
  urldate = {2023-09-26},
  abstract = {Groundwater level forecasting is a useful tool for a more efficient and sustainable groundwater resource management. Developing models that can accurately reproduce groundwater level response to meteorological conditions can lead to a better understanding of the groundwater resource availability. Here an autoregressive neural network (NNARx) approach is proposed and compared with autoregressive linear models with exogenous input (ARx) in order to forecast groundwater level in an aquifer system where a linear groundwater level response to recharge by rainfall is observed. A well known problem regarding neural networks consists in the high risk of overfitting. Here, three NNARx model were trained using different methods to avoid overfitting: Early stopping, Bayesian regularization and a combination of both. The results show that on the short term forecasting (up to 15\,days) the performance of NNARx and ARx are comparable but the ARx model generalizes better, while the NNARx trained with Bayesian regularization outperforms the linear models and the other NNARx models on longer scenarios on the test set. As linear models are less time demanding and do not require high computational power, they can be considered as suitable tools for short term groundwater level forecasting in linear systems while when longer scenarios are needed neural networks can be considered more reliable, and training them with Bayesian regularization allows to minimize the risk of overfitting.},
  keywords = {Bayesian regularization,Groundwater level forecasting,Linear model,Neural networks,Overfitting}
}

@article{zhangEstimatingDynamicSolar2022,
  title = {Estimating Dynamic Solar Gains from On-Site Measured Data: {{An ARX}} Modelling Approach},
  shorttitle = {Estimating Dynamic Solar Gains from On-Site Measured Data},
  author = {Zhang, Xiang and Saelens, Dirk and Roels, Staf},
  year = {2022},
  month = sep,
  journal = {Applied Energy},
  volume = {321},
  pages = {119278},
  issn = {0306-2619},
  doi = {10.1016/j.apenergy.2022.119278},
  urldate = {2023-11-07},
  abstract = {On-site measured data in combination with statistical methods is used more and more to assess the actual performance of a building and to develop simplified models that can be used in model predictive control, fault detection, and optimization of energy grids. Most of the methods are based on a simplified heat balance of the building. For this heat balance, solar gains, referring to the part of energy supplied by the sun, are a vital factor. Gauging solar gains and their time dependency in practice is, however, challenging. Most models assume the solar aperture of the building as an invariable property, although it is highly dependent on the sun position during the time of the day and year. In this study a statistical modeling method, based on in-situ measurement data, is developed to estimate the time-varying solar gains more precisely. The method integrates basis splines (B-splines) into an ARX model (Auto-regressive with eXogenous input). Verified by white-box model simulation outcomes, it is demonstrated that this B-splines integrated ARX modelling approach can reflect the key information of the dynamic features ofthe solar gainsto a large extent. Most importantly, only a limited size of in-situ measurement data of high-frequency is required in this method, indicating its good efficiency in dynamic solar gain estimation.},
  keywords = {Autoregressive with exogenous input (ARX) model,B-splines,Black-box model,Data-driven method,Dynamic solar aperture ()}
}

@article{zhengEstimationARXParametric2007,
  title = {Estimation of {{ARX}} Parametric Model in Regional Economic Systems},
  author = {Zheng, Shi and Zheng, Wen and Jin, Xia},
  year = {2007},
  journal = {Systems Engineering},
  volume = {10},
  number = {4},
  pages = {290--296},
  issn = {1520-6858},
  doi = {10.1002/sys.20077},
  urldate = {2023-11-07},
  abstract = {The purpose of this paper is to show how an economic system model can be selected that would allow a regional authority to control the economy of a region. This paper presents a new idea of system identification method based on fuzzy evaluation logic. It applies parametric estimation in analyzing the data of the inputs and the outputs in a nonlinear system, and then selects a regional economic system model from several nonlinear models by means of the system identification method. Due to the lower explanatory power of the present models, this paper focuses on the shortcomings of the present model based on a simple mechanism methodology which has a complicated structure and lots of variables. An alternative is to construct an ARX model with systematic consideration. The advantage of ARX model is simple, with few variables as well as being actually data-based. The result is that ARX117 model is the best one, and the IV Estimation Method is the one we suggest for use. Since this ARX model can reflect the complex characteristics of a regional economic system by integration of experts' knowledge, we can design the most reasonable fuzzy controller for a regional economic system. \textcopyright{} 2007 Wiley Periodicals, Inc. Syst Eng 10: 290\textendash 296, 2007},
  langid = {english},
  keywords = {ARX Parametric Model,nonlinear system,system identification}
}

@article{solinHilbertSpaceMethods2020,
  title = {Hilbert Space Methods for Reduced-Rank {{Gaussian}} Process Regression},
  author = {Solin, Arno and Särkkä, Simo},
  year = {2020},
  journal = {Statistics and Computing},
  volume = {30},
  number = {2},
  pages = {419--446},
  doi = {10.1007/s11222-019-09886-w}
}

@article{roberts2013gaussian,
  title={Gaussian processes for time-series modelling},
  author={Roberts, Stephen and Osborne, Michael and Ebden, Mark and Reece, Steven and Gibson, Neale and Aigrain, Suzanne},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={371},
  number={1984},
  pages={20110550},
  year={2013},
  publisher={The Royal Society Publishing}
}

@inproceedings{solin2014explicit,
  title={Explicit link between periodic covariance functions and state space models},
  author={Solin, Arno and S{\"a}rkk{\"a}, Simo},
  booktitle={Artificial Intelligence and Statistics},
  pages={904--912},
  year={2014},
  organization={PMLR}
}

@article{riutort2023practical,
  title={Practical Hilbert space approximate Bayesian Gaussian processes for probabilistic programming},
  author={Riutort-Mayol, Gabriel and B{\"u}rkner, Paul-Christian and Andersen, Michael R and Solin, Arno and Vehtari, Aki},
  journal={Statistics and Computing},
  volume={33},
  number={1},
  pages={17},
  year={2023},
  publisher={Springer}
}

@article{kohzadi1996comparison,
  title={A comparison of artificial neural network and time series models for forecasting commodity prices},
  author={Kohzadi, Nowrouz and Boyd, Milton S and Kermanshahi, Bahman and Kaastra, Iebeling},
  journal={Neurocomputing},
  volume={10},
  number={2},
  pages={169--181},
  year={1996},
  publisher={Elsevier}
}

@article{hauzenberger2022enhanced,
  title = {Bayesian Neural Networks for Macroeconomic Analysis},
  author = {Hauzenberger, Niko and Huber, Florian and Klieber, Karin and Marcellino, Massimiliano},
  year = {2024},
  month = sep,
  journal = {Journal of Econometrics},
  pages = {105843},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2024.105843},
  langid = {english}
}


@article{maasoumi1994artificial,
  title={Artificial neural networks for some macroeconomic series: a first report},
  author={Maasoumi, Esfandiar and Khotanzed, A and Abaye, A},
  journal={Econometric Reviews},
  volume={13},
  number={1},
  pages={105--122},
  year={1994},
  publisher={Taylor \& Francis}
}

@article{pierce1979r,
  title={R 2 measures for time series},
  author={Pierce, David A},
  journal={Journal of the American Statistical Association},
  volume={74},
  number={368},
  pages={901--910},
  year={1979},
  publisher={Taylor \& Francis}
}

@article{fruhwirth2010stochastic,
  title={Stochastic model specification search for {Gaussian} and partial non-{Gaussian} state space models},
  author={Fr{\"u}hwirth-Schnatter, Sylvia and Wagner, Helga},
  journal={Journal of Econometrics},
  volume={154},
  number={1},
  pages={85--100},
  year={2010},
  publisher={Elsevier}
}

@article{chan2020reducing,
  title={Reducing the state space dimension in a large {TVP-VAR}},
  author={Chan, Joshua CC and Eisenstat, Eric and Strachan, Rodney W},
  journal={Journal of Econometrics},
  volume={218},
  number={1},
  pages={105--118},
  year={2020},
  publisher={Elsevier}
}

@article{chan2023bayesian,
  title={Bayesian state space models in macroeconometrics},
  author={Chan, Joshua CC and Strachan, Rodney W},
  journal={Journal of Economic Surveys},
  volume={37},
  number={1},
  pages={58--75},
  year={2023},
  publisher={Wiley Online Library}
}

@book{sarkka2023bayesian,
  title={Bayesian filtering and smoothing},
  author={S{\"a}rkk{\"a}, Simo and Svensson, Lennart},
  volume={17},
  year={2023},
  publisher={Cambridge University Press}
}

@article{kitagawa1984smoothness,
  title={A smoothness priors--state space modeling of time series with trend and seasonality},
  author={Kitagawa, Genshiro and Gersch, Will},
  journal={Journal of the American Statistical Association},
  volume={79},
  number={386},
  pages={378--389},
  year={1984},
  publisher={Taylor \& Francis}
}

@article{cadonna2020triple,
  title={Triple the gamma—a unifying shrinkage prior for variance and variable selection in sparse state space and {TVP} models},
  author={Cadonna, Annalisa and Fr{\"u}hwirth-Schnatter, Sylvia and Knaus, Peter},
  journal={Econometrics},
  volume={8},
  number={2},
  pages={20},
  year={2020},
  publisher={MDPI}
}

@article{kim1999state,
  title={State-space models with regime switching: classical and Gibbs-sampling approaches with applications},
  author={Kim, Chang-Jin and Nelson, Charles R and others},
  journal={MIT Press Books},
  volume={1},
  year={1999},
  publisher={The MIT press}
}

@book{hamilton2020time,
  title={Time series analysis},
  author={Hamilton, James D},
  year={2020},
  publisher={Princeton University Press}
}

@article{scott2014predicting,
  title={Predicting the present with {Bayesian} structural time series},
  author={Scott, Steven L and Varian, Hal R},
  journal={International Journal of Mathematical Modelling and Numerical Optimisation},
  volume={5},
  number={1-2},
  pages={4--23},
  year={2014},
  publisher={Inderscience Publishers Ltd}
}

@article{qiu2018multivariate,
  title={Multivariate Bayesian Structural Time Series Model.},
  author={Qiu, Jinwen and Jammalamadaka, S Rao and Ning, Ning},
  journal={J. Mach. Learn. Res.},
  volume={19},
  number={1},
  pages={2744--2776},
  year={2018}
}

@article{brodersen2015inferring,
  title={Inferring causal impact using Bayesian structural time-series models},
  author={Brodersen, Kay H and Gallusser, Fabian and Koehler, Jim and Remy, Nicolas and Scott, Steven L},
  year={2015},
  journal={Annals of Applied Statistics},
  pages={247--274},
  volume={9},
  number={1}
}

@article{mclatchie_efficient_2023,
  title = {Efficient Estimation and Correction of Selection-Induced Bias with Order Statistics},
  author = {McLatchie, Yann and Vehtari, Aki},
  year = {2024},
  month = jun,
  journal = {Statistics and Computing},
  volume = {34},
  number = {4},
  pages = {132},
  issn = {1573-1375},
  doi = {10.1007/s11222-024-10442-4},
  urldate = {2024-11-20},
  abstract = {Model selection aims to identify a sufficiently well performing model that is possibly simpler than the most complex model among a pool of candidates. However, the decision-making process itself can inadvertently introduce non-negligible bias when the cross-validation estimates of predictive performance are marred by excessive noise. In finite data regimes, cross-validated estimates can encourage the statistician to select one model over another when it is not actually better for future data. While this bias remains negligible in the case of few models, when the pool of candidates grows, and model selection decisions are compounded (as in step-wise selection), the expected magnitude of selection-induced bias is likely to grow too. This paper introduces an efficient approach to estimate and correct selection-induced bias based on order statistics. Numerical experiments demonstrate the reliability of our approach in estimating both selection-induced bias and over-fitting along compounded model selection decisions, with specific application to forward search. This work represents a light-weight alternative to more computationally expensive approaches to correcting selection-induced bias, such as nested cross-validation and the bootstrap. Our approach rests on several theoretic assumptions, and we provide a diagnostic to help understand when these may not be valid and when to fall back on safer, albeit more computationally expensive approaches. The accompanying code facilitates its practical implementation and fosters further exploration in this area.},
  langid = {english},
  keywords = {Artificial Intelligence,Bayesian model selection,Bias correction,Cross-validation,Selection-induced bias}
}


@article{harvey1993detrending,
  title={Detrending, stylized facts and the business cycle},
  author={Harvey, Andrew C and Jaeger, Albert},
  journal={Journal of applied econometrics},
  volume={8},
  number={3},
  pages={231--247},
  year={1993},
  publisher={Wiley Online Library}
}

@manual{stan,
  title = {Stan {{User's Guide}} and {{Reference Manual}}},
  author = {{Stan Development Team}},
  year = {2025},
  url = {https://mc-stan.org},
  version = {2.36}
}

@article{saitoBayesianModelSelection2010,
  title = {Bayesian Model Selection for {{ARX}} Models and Its Application to Structural Health Monitoring},
  author = {Saito, Tomoo and Beck, James L.},
  year = {2010},
  journal = {Earthquake Engineering \& Structural Dynamics},
  volume = {39},
  number = {15},
  pages = {1737--1759},
  issn = {1096-9845},
  doi = {10.1002/eqe.1006},
  urldate = {2023-09-26},
  abstract = {A Bayesian framework for model order selection of auto-regressive exogenous (ARX) models is developed and applied to actual earthquake response data obtained by the structural health monitoring system of a high-rise building. The model orders of ARX models are selected appropriately by the Bayesian framework, and differ significantly from the optimal order estimated by AIC; in fact, in many cases AIC does not even give an optimal order. A method is also proposed for consistently selecting the same ‘genuine’ modes of interest from the whole set of modes corresponding to each of the identified models from a sequence of earthquake records. In the identification analysis based on building response records from 43 earthquakes over 9 years, the modal parameters of the first four modes in each horizontal direction are estimated appropriately in all cases, showing that the developed methods are effective and robust. As the estimates of natural frequency depend significantly on the response amplitude, they are compensated by an empirical correction so that the influence of the response amplitude is removed. The compensated natural frequencies are much more stable over the nine-year period studied, indicating that the building had no significant change in its global dynamic characteristics during this period. Copyright © 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {ARX model,Bayesian model class selection,earthquake response,modal parameters,structural health monitoring,system identification}
}


@book{lutkepohl_new_2005,
	title = {New {Introduction} to {Multiple} {Time} {Series} {Analysis}},
	isbn = {9783540277521},
	abstract = {When I worked on my Introduction to Multiple Time Series Analysis (Lutk ̈ ̈- pohl (1991)), a suitable textbook for this ?eld was not available. Given the great importance these methods have gained in applied econometric work, it is perhaps not surprising in retrospect that the book was quite successful. Now, almost one and a half decades later the ?eld has undergone substantial development and, therefore, the book does not cover all topics of my own courses on the subject anymore. Therefore, I started to think about a serious revision of the book when I moved to the European University Institute in Florence in 2002. Here in the lovely hills of ToscanyIhadthetimetothink about bigger projects again and decided to prepare a substantial revision of my previous book. Because the label Second Edition was already used for a previous reprint of the book, I decided to modify the title and thereby hope to signal to potential readers that signi?cant changes have been made relative to my previous multiple time series book.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Lütkepohl, Helmut},
	month = dec,
	year = {2005},
	keywords = {Business \& Economics / Econometrics, Business \& Economics / General, Business \& Economics / Statistics, Mathematics / Applied, Mathematics / Probability \& Statistics / General, Technology \& Engineering / Engineering (General)},
}

@book{harvey_forecasting_1990,
	title = {Forecasting, {Structural} {Time} {Series} {Models} and the {Kalman} {Filter}},
	isbn = {9780521405737},
	abstract = {This book provides a synthesis of concepts and materials that ordinarily appear separately in time series and econometrics literature, presenting a comprehensive review of both theoretical and applied concepts. Perhaps the most novel feature of the book is its use of Kalman filtering together with econometric and time series methodology. From a technical point of view, state space models and the Kalman filter play a key role in the statistical treatment of structural time series models. This technique was originally developed in control engineering but is becoming increasingly important in economics and operations research. The book is primarily concerned with modeling economic and social time series and with addressing the special problems that the treatment of such series pose.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Harvey, Andrew C.},
	year = {1990},
	keywords = {Business \& Economics / Econometrics},
}

@article{talts_validating_2020,
	title = {Validating {Bayesian} {Inference} {Algorithms} with {Simulation}-{Based} {Calibration}},
	url = {http://arxiv.org/abs/1804.06788},
	abstract = {Verifying the correctness of Bayesian computation is challenging. This is especially true for complex models that are common in practice, as these require sophisticated model implementations and algorithms. In this paper we introduce {\textbackslash}emph\{simulation-based calibration\} (SBC), a general procedure for validating inferences from Bayesian algorithms capable of generating posterior samples. This procedure not only identifies inaccurate computation and inconsistencies in model implementations but also provides graphical summaries that can indicate the nature of the problems that arise. We argue that SBC is a critical part of a robust Bayesian workflow, as well as being a useful tool for those developing computational algorithms and statistical software.},
	urldate = {2021-11-17},
	journal = {arXiv:1804.06788 [stat]},
	author = {Talts, Sean and Betancourt, Michael and Simpson, Daniel and Vehtari, Aki and Gelman, Andrew},
	month = oct,
	year = {2020},
	note = {arXiv: 1804.06788},
	keywords = {Statistics - Methodology},
}

@misc{modrak_simulation-based_2022,
	title = {Simulation-{Based} {Calibration} {Checking} for {Bayesian} {Computation}: {The} {Choice} of {Test} {Quantities} {Shapes} {Sensitivity}},
	shorttitle = {Simulation-{Based} {Calibration} {Checking} for {Bayesian} {Computation}},
	url = {http://arxiv.org/abs/2211.02383},
	abstract = {Simulation-based calibration checking (SBC) is a practical method to validate computationally-derived posterior distributions or their approximations. In this paper, we introduce a new variant of SBC to alleviate several known problems. Our variant allows the user to in principle detect any possible issue with the posterior, while previously reported implementations could never detect large classes of problems including when the posterior is equal to the prior. This is made possible by including additional data-dependent test quantities when running SBC. We argue and demonstrate that the joint likelihood of the data is an especially useful test quantity. Some other types of test quantities and their theoretical and practical benefits are also investigated. We support our recommendations with numerical case studies on a multivariate normal example and theoretical analysis of SBC, thereby providing a more complete understanding of the underlying statistical mechanisms. From the theoretical side, we also bring attention to a relatively common mistake in the literature and clarify the difference between SBC and checks based on the data-averaged posterior. The SBC variant introduced in this paper is implemented in the SBC R package.},
	urldate = {2022-12-21},
	publisher = {arXiv},
	author = {Modrák, Martin and Moon, Angie H. and Kim, Shinyoung and Bürkner, Paul and Huurre, Niko and Faltejsková, Kateřina and Gelman, Andrew and Vehtari, Aki},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02383 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{sailynoja_graphical_2021,
	title = {Graphical {Test} for {Discrete} {Uniformity} and its {Applications} in {Goodness} of {Fit} {Evaluation} and {Multiple} {Sample} {Comparison}},
	url = {http://arxiv.org/abs/2103.10522},
	abstract = {Assessing goodness of fit to a given distribution plays an important role in computational statistics. The Probability integral transformation (PIT) can be used to convert the question of whether a given sample originates from a reference distribution into a problem of testing for uniformity. We present new simulation and optimization based methods to obtain simultaneous confidence bands for the whole empirical cumulative distribution function (ECDF) of the PIT values under the assumption of uniformity. Simultaneous confidence bands correspond to such confidence intervals at each point that jointly satisfy a desired coverage. These methods can also be applied in cases where the reference distribution is represented only by a finite sample. The confidence bands provide an intuitive ECDF-based graphical test for uniformity, which also provides useful information on the quality of the discrepancy. We further extend the simulation and optimization methods to determine simultaneous confidence bands for testing whether multiple samples come from the same underlying distribution. This multiple sample comparison test is especially useful in Markov chain Monte Carlo convergence diagnostics. We provide numerical experiments to assess the properties of the tests using both simulated and real world data and give recommendations on their practical application in computational statistics workflows.},
	urldate = {2022-12-21},
	publisher = {arXiv},
	author = {Säilynoja, Teemu and Bürkner, Paul-Christian and Vehtari, Aki},
	month = nov,
	year = {2021},
	note = {arXiv:2103.10522 [stat]},
	keywords = {Statistics - Methodology},
}

@article{carriero_realtime_2015,
	title = {Realtime nowcasting with a {Bayesian} mixed frequency model with stochastic volatility},
	volume = {178},
	issn = {09641998},
	doi = {10.1111/rssa.12092},
	language = {en},
	number = {4},
	urldate = {2023-02-15},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Carriero, Andrea and Clark, Todd E. and Marcellino, Massimiliano},
	month = oct,
	year = {2015},
	pages = {837--862},
}

@article{carriero_bayesian_2015,
	title = {Bayesian {VARs}: {Specification} {Choices} and {Forecast} {Accuracy}},
	volume = {30},
	issn = {08837252},
	shorttitle = {Bayesian {VARs}},
	doi = {10.1002/jae.2315},
	language = {en},
	number = {1},
	urldate = {2023-02-15},
	journal = {Journal of Applied Econometrics},
	author = {Carriero, Andrea and Clark, Todd E. and Marcellino, Massimiliano},
	month = jan,
	year = {2015},
	pages = {46--73},
}

@article{chan_minnesota-type_2021,
	title = {Minnesota-type adaptive hierarchical priors for large {Bayesian} {VARs}},
	volume = {37},
	issn = {01692070},
	doi = {10.1016/j.ijforecast.2021.01.002},
	language = {en},
	number = {3},
	urldate = {2023-02-15},
	journal = {International Journal of Forecasting},
	author = {Chan, Joshua C.C.},
	month = jul,
	year = {2021},
	pages = {1212--1226},
}

@article{gneiting_assessing_2008,
	title = {Assessing probabilistic forecasts of multivariate quantities, with an application to ensemble predictions of surface winds},
	volume = {17},
	issn = {1133-0686, 1863-8260},
	doi = {10.1007/s11749-008-0114-x},
	language = {en},
	number = {2},
	urldate = {2023-02-14},
	journal = {TEST},
	author = {Gneiting, Tilmann and Stanberry, Larissa I. and Grimit, Eric P. and Held, Leonhard and Johnson, Nicholas A.},
	month = aug,
	year = {2008},
	pages = {211--235},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2023-02-13},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	pages = {359--378},
}

@misc{vehtari_pareto_2022,
	title = {Pareto {Smoothed} {Importance} {Sampling}},
	url = {http://arxiv.org/abs/1507.02646},
	doi = {10.48550/arXiv.1507.02646},
	abstract = {Importance weighting is a general way to adjust Monte Carlo integration to account for draws from the wrong distribution, but the resulting estimate can be highly variable when the importance ratios have a heavy right tail. This routinely occurs when there are aspects of the target distribution that are not well captured by the approximating distribution, in which case more stable estimates can be obtained by modifying extreme importance ratios. We present a new method for stabilizing importance weights using a generalized Pareto distribution fit to the upper tail of the distribution of the simulated importance ratios. The method, which empirically performs better than existing methods for stabilizing importance sampling estimates, includes stabilized effective sample size estimates, Monte Carlo error estimates, and convergence diagnostics. The presented Pareto \${\textbackslash}hat\{k\}\$ finite sample convergence rate diagnostic is useful for any Monte Carlo estimator.},
	urldate = {2023-02-13},
	publisher = {arXiv},
	author = {Vehtari, Aki and Simpson, Daniel and Gelman, Andrew and Yao, Yuling and Gabry, Jonah},
	month = aug,
	year = {2022},
	note = {arXiv:1507.02646 [stat]},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@article{burkner_approximate_2020,
	title = {Approximate leave-future-out cross-validation for {Bayesian} time series models},
	volume = {90},
	issn = {0094-9655, 1563-5163},
	doi = {10.1080/00949655.2020.1783262},
	language = {en},
	number = {14},
	urldate = {2023-02-13},
	journal = {Journal of Statistical Computation and Simulation},
	author = {Bürkner, Paul-Christian and Gabry, Jonah and Vehtari, Aki},
	month = sep,
	year = {2020},
	pages = {2499--2523},
}

@article{carriero_bayesian_2015-1,
	title = {Bayesian {VARs}: {Specification} {Choices} and {Forecast} {Accuracy}: {BAYESIAN} {VARS}},
	volume = {30},
	issn = {08837252},
	shorttitle = {Bayesian {VARs}},
	doi = {10.1002/jae.2315},
	language = {en},
	number = {1},
	urldate = {2023-02-13},
	journal = {Journal of Applied Econometrics},
	author = {Carriero, Andrea and Clark, Todd E. and Marcellino, Massimiliano},
	month = jan,
	year = {2015},
	pages = {46--73},
}

@book{box_time_1994,
	address = {USA},
	edition = {3rd},
	title = {Time {Series} {Analysis}: {Forecasting} and {Control}},
	isbn = {9780130607744},
	shorttitle = {Time {Series} {Analysis}},
	abstract = {From the Publisher: This is a complete revision of a classic, seminal, and authoritative book that has been the model for most books on the topic written since 1970. It focuses on practical techniques throughout, rather than a rigorous mathematical treatment of the subject. It explores the building of stochastic (statistical) models for time series and their use in important areas of application —forecasting, model specification, estimation, and checking, transfer function modeling of dynamic relationships, modeling the effects of intervention events, and process control. Features sections on: recently developed methods for model specification, such as canonical correlation analysis and the use of model selection criteria; results on testing for unit root nonstationarity in ARIMA processes; the state space representation of ARMA models and its use for likelihood estimation and forecasting; score test for model checking; and deterministic components and structural components in time series models and their estimation based on regression-time series model methods.},
	publisher = {Prentice Hall PTR},
	author = {Box, George Edward Pelham and Jenkins, Gwilym M.},
	year = {1994},
}

@article{yule_vii_1927,
	title = {{VII}. {On} a method of investigating periodicities disturbed series, with special reference to {Wolfer}'s sunspot numbers},
	volume = {226},
	issn = {0264-3952, 2053-9258},
	doi = {10.1098/rsta.1927.0007},
	abstract = {If we take a curve representing a simple harmonic function of the time, and superpose on the ordinates 
              small 
              random errors, the only effect is to make the graph somewhat irregular, leaving the suggestion of periodicity still quite clear to the eye. Fig. 1 ( 
              a 
              ) shows such a curve, the random errors having been determined by the throws of dice. If the errors are increased in magnitude, as in fig. 1 ( 
              b 
              ), the graph becomes more irregular, the suggestion of periodicity more obscure, and we have only sufficiently to increase the “errors” to mask completely any appearance of periodicity. But, however large the errors, periodogram analysis is applicable to such a curve, and, given a sufficient number of periods, should yield a close approximation to the period and amplitude of the underlying harmonic function. When periodogram analysis is applied to data respecting any physical phenomenon in the expectation of eliciting one or more true periodicities, there is usually, as it seems to me, a tendency to start from the initial hypothesis that the periodicity or periodicities are masked solely by such more or less random 
              superposed fluctuations 
              — fluctuations which do not in any way disturb the steady course of the underlying periodic function or functions. It is true that the periodogram itself will indicate the truth or otherwise of the hypothesis made, but there seems no reason for assuming it to be the hypothesis most likely a 
              priori 
              .},
	language = {en},
	number = {636-646},
	urldate = {2023-02-07},
	journal = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	author = {Yule, George Udny},
	month = jan,
	year = {1927},
	pages = {267--298},
}

@article{walker_periodicity_1931,
	title = {On periodicity in series of related terms},
	volume = {131},
	issn = {0950-1207, 2053-9150},
	doi = {10.1098/rspa.1931.0069},
	language = {en},
	number = {818},
	urldate = {2023-02-07},
	journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
	author = {Walker, Gilbert Thomas},
	month = jun,
	year = {1931},
	pages = {518--532},
}

@article{gelman_r-squared_2019,
	title = {R-squared for {Bayesian} {Regression} {Models}},
	volume = {73},
	issn = {0003-1305, 1537-2731},
	doi = {10.1080/00031305.2018.1549100},
	language = {en},
	number = {3},
	urldate = {2023-02-07},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari, Aki},
	month = jul,
	year = {2019},
	pages = {307--309},
}

@misc{arno_solin_stochastic_nodate,
	title = {Stochastic {Differential} {Equation} {Methods} for {Spatio}-{Temporal} {Gaussian} {Process} {Regression}},
	author = {{Arno Solin}},
}

@inproceedings{duchi_efficient_2008,
	address = {New York, NY, USA},
	series = {{ICML} '08},
	title = {Efficient projections onto the l1-ball for learning in high dimensions},
	isbn = {978-1-60558-205-4},
	doi = {10.1145/1390156.1390191},
	abstract = {We describe efficient algorithms for projecting a vector onto the l1-ball. We present two methods for projection. The first performs exact projection in O(n) expected time, where n is the dimension of the space. The second works on vectors k of whose elements are perturbed outside the l1-ball, projecting in O(k log(n)) time. This setting is especially useful for online learning in sparse feature spaces such as text categorization applications. We demonstrate the merits and effectiveness of our algorithms in numerous batch and online learning tasks. We show that variants of stochastic gradient projection methods augmented with our efficient projection procedures outperform interior point methods, which are considered state-of-the-art optimization techniques. We also show that in online settings gradient updates with l1 projections outperform the exponentiated gradient algorithm while obtaining models with high degrees of sparsity.},
	urldate = {2022-12-03},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Duchi, John and Shalev-Shwartz, Shai and Singer, Yoram and Chandra, Tushar},
	month = jul,
	year = {2008},
	note = {tex.ids= duchiEfficientProjectionsL1ball2008},
	pages = {272--279},
}

@misc{sen_constrained_2022,
	title = {Constrained inference through posterior projections},
	url = {http://arxiv.org/abs/1812.05741},
	abstract = {Bayesian approaches are appealing for constrained inference problems by allowing a probabilistic characterization of uncertainty, while providing a computational machinery for incorporating complex constraints in hierarchical models. However, the usual Bayesian strategy of placing a prior on the constrained space and conducting posterior computation with Markov chain Monte Carlo algorithms is often intractable. An alternative is to conduct inference for a less constrained posterior and project samples to the constrained space through a minimal distance mapping. We formalize and provide a unifying framework for such posterior projections. For theoretical tractability, we initially focus on constrained parameter spaces corresponding to closed and convex subsets of the original space. We then consider non-convex Stiefel manifolds. We provide a general formulation of projected posteriors in a Bayesian decision-theoretic framework. We show that asymptotic properties of the unconstrained posterior are transferred to the projected posterior, leading to asymptotically correct credible intervals. We demonstrate numerically that projected posteriors can have better performance that competitor approaches in real data examples.},
	urldate = {2023-02-06},
	publisher = {arXiv},
	author = {Sen, Deborshee and Patra, Sayan and Dunson, David},
	month = jan,
	year = {2022},
	note = {arXiv:1812.05741 [stat]},
	keywords = {Statistics - Methodology},
}

@article{sorbye_penalised_2017,
	title = {Penalised {Complexity} {Priors} for {Stationary} {Autoregressive} {Processes}: {PC} priors for {AR} processes},
	volume = {38},
	issn = {01439782},
	shorttitle = {Penalised {Complexity} {Priors} for {Stationary} {Autoregressive} {Processes}},
	doi = {10.1111/jtsa.12242},
	language = {en},
	number = {6},
	urldate = {2023-02-06},
	journal = {Journal of Time Series Analysis},
	author = {Sørbye, Sigrunn Holbek and Rue, Håvard},
	month = nov,
	year = {2017},
	pages = {923--935},
}

@book{boyd_convex_2004,
	edition = {1},
	title = {Convex {Optimization}},
	isbn = {9780521833783 9780511804441},
	urldate = {2023-01-28},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	month = mar,
	year = {2004},
	doi = {10.1017/CBO9780511804441},
}

@article{fuglstad_intuitive_2020,
	title = {Intuitive {Joint} {Priors} for {Variance} {Parameters}},
	volume = {15},
	issn = {1936-0975},
	doi = {10.1214/19-BA1185},
	number = {4},
	urldate = {2023-01-28},
	journal = {Bayesian Analysis},
	author = {Fuglstad, Geir-Arne and Hem, Ingeborg Gullikstad and Knight, Alexander and Rue, Håvard and Riebler, Andrea},
	month = dec,
	year = {2020},
}

@article{yanchenko_r2d2_2023,
	  title = {Spatial Regression Modeling via the {{R2D2}} Framework},
  author = {Yanchenko, Eric and Bondell, Howard D. and Reich, Brian J.},
  year = {2024},
  journal = {Environmetrics},
  volume = {35},
  number = {2},
  pages = {e2829},
  issn = {1099-095X},
  doi = {10.1002/env.2829},
  copyright = {{\copyright} 2023 The Authors. Environmetrics published by John Wiley \& Sons Ltd.},
  langid = {english}
}

@article{roberts_gaussian_2013,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper, we offer a gentle introduction to Gaussian processes for time-series data analysis. The conceptual framework of Bayesian modelling for time-series data is discussed and the foundations of Bayesian non-parametric modelling presented for Gaussian processes. We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
	number = {1984},
	urldate = {2023-01-26},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
	month = feb,
	year = {2013},
	keywords = {Bayesian modelling, Gaussian processes, time-series analysis},
	pages = {20110550},
}

@article{huber_adaptive_2019,
	title = {Adaptive {Shrinkage} in {Bayesian} {Vector} {Autoregressive} {Models}},
	volume = {37},
	issn = {0735-0015},
	doi = {10.1080/07350015.2016.1256217},
	abstract = {Vector autoregressive (VAR) models are frequently used for forecasting and impulse response analysis. For both applications, shrinkage priors can help improving inference. In this article, we apply the Normal-Gamma shrinkage prior to the VAR with stochastic volatility case and derive its relevant conditional posterior distributions. This framework imposes a set of normally distributed priors on the autoregressive coefficients and the covariance parameters of the VAR along with Gamma priors on a set of local and global prior scaling parameters. In a second step, we modify this prior setup by introducing another layer of shrinkage with scaling parameters that push certain regions of the parameter space to zero. Two simulation exercises show that the proposed framework yields more precise estimates of model parameters and impulse response functions. In addition, a forecasting exercise applied to U.S. data shows that this prior performs well relative to other commonly used specifications in terms of point and density predictions. Finally, performing structural inference suggests that responses to monetary policy shocks appear to be reasonable.},
	number = {1},
	urldate = {2023-01-26},
	journal = {Journal of Business \& Economic Statistics},
	author = {Huber, Florian and Feldkircher, Martin},
	month = jan,
	year = {2019},
	keywords = {Density predictions, Hierarchical modeling, Normal-Gamma prior, Stochastic volatility.},
	pages = {27--39},
}

@article{nelson_interpretation_1976,
	title = {The {Interpretation} of {R}2 in {Autoregressive}-{Moving} {Average} {Time} {Series} {Models}},
	volume = {30},
	issn = {0003-1305},
	doi = {10.1080/00031305.1976.10479171},
	number = {4},
	urldate = {2023-01-26},
	journal = {The American Statistician},
	author = {Nelson, Charles R.},
	month = nov,
	year = {1976},
	pages = {175--180},
}

@misc{gruber_forecasting_2022,
	title = {Forecasting macroeconomic data with {Bayesian} {VARs}: {Sparse} or dense? {It} depends!},
	shorttitle = {Forecasting macroeconomic data with {Bayesian} {VARs}},
	url = {http://arxiv.org/abs/2206.04902},
	doi = {10.48550/arXiv.2206.04902},
	abstract = {Vectorautogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinking priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the recently developed \$R{\textasciicircum}2\$-induced Dirichlet-decomposition prior to the VAR framework and compare it to refinements of well-known priors in the VAR literature. In addition, we develop a semi-global framework, in which we replace the traditional global shrinkage parameter with group specific shrinkage parameters. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing "Illusion of Sparsity" debate. We find that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames; dynamic model averaging, however, can combine the merits of both worlds.},
	urldate = {2022-12-14},
	publisher = {arXiv},
	author = {Gruber, Luis and Kastner, Gregor},
	month = aug,
	year = {2022},
	note = {arXiv:2206.04902 [econ, stat]},
	keywords = {Economics - Econometrics, Statistics - Applications, Statistics - Methodology},
}

@article{yanchenko_r2d2_2021,
  title = {The {{R2D2 Prior}} for {{Generalized Linear Mixed Models}}},
  author = {Yanchenko, Eric and Bondell, Howard D. and Reich, Brian J.},
  year = {2024},
  journal = {The American Statistician},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2024.2352010}
}

@article{koop_forecasting_2013,
	title = {Forecasting with {Medium} and {Large} {Bayesian} {VAR}s},
	volume = {28},
	issn = {08837252},
	shorttitle = {Forecasting with {Medium} and {Large} {Bayesian} {VARS}},
	doi = {10.1002/jae.1270},
	language = {en},
	number = {2},
	urldate = {2022-12-03},
	journal = {Journal of Applied Econometrics},
	author = {Koop, Gary M.},
	month = mar,
	year = {2013},
	pages = {177--203},
}

@article{barthe_probabilistic_2005,
	title = {A probabilistic approach to the geometry of the \${\textbackslash}ell\_p{\textasciicircum}n\$-ball},
	volume = {33},
	issn = {0091-1798},
	url = {http://arxiv.org/abs/math/0503650},
	doi = {10.1214/009117904000000874},
	abstract = {This article investigates, by probabilistic methods, various geometric questions on B\_p{\textasciicircum}n, the unit ball of {\textbackslash}ell\_p{\textasciicircum}n. We propose realizations in terms of independent random variables of several distributions on B\_p{\textasciicircum}n, including the normalized volume measure. These representations allow us to unify and extend the known results of the sub-independence of coordinate slabs in B\_p{\textasciicircum}n. As another application, we compute moments of linear functionals on B\_p{\textasciicircum}n, which gives sharp constants in Khinchine's inequalities on B\_p{\textasciicircum}n and determines the {\textbackslash}psi\_2-constant of all directions on B\_p{\textasciicircum}n. We also study the extremal values of several Gaussian averages on sections of B\_p{\textasciicircum}n (including mean width and {\textbackslash}ell-norm), and derive several monotonicity results as p varies. Applications to balancing vectors in {\textbackslash}ell\_2 and to covering numbers of polyhedra complete the exposition.},
	number = {2},
	urldate = {2022-12-03},
	journal = {The Annals of Probability},
	author = {Barthe, Franck and Guedon, Olivier and Mendelson, Shahar and Naor, Assaf},
	month = mar,
	year = {2005},
	note = {arXiv:math/0503650},
	keywords = {60E15, 52A20, 52A38, 52A40. (Primary), Mathematics - Probability},
}

@article{gelman_bayesian_1996,
	title = {Bayesian {Model}-{Building} {By} {Pure} {Thought}: {Some} {Principles} and {Examples}},
	volume = {6},
	number = {1},
	journal = {Statistica Sinica},
	author = {Gelman, Andrew},
	month = jan,
	year = {1996},
}

@article{neyman_theory_1940,
	title = {Theory of {Probability}.},
	volume = {35},
	issn = {01621459},
	doi = {10.2307/2279284},
	number = {211},
	urldate = {2022-12-04},
	journal = {Journal of the American Statistical Association},
	author = {Neyman, J. and Jeffreys, Harold},
	month = sep,
	year = {1940},
	pages = {558},
}

@article{gelman_holes_2021,
	title = {Holes in {Bayesian} {Statistics}},
	volume = {48},
	issn = {0954-3899, 1361-6471},
	url = {http://arxiv.org/abs/2002.06467},
	doi = {10.1088/1361-6471/abc3a5},
	abstract = {Every philosophy has holes, and it is the responsibility of proponents of a philosophy to point out these problems. Here are a few holes in Bayesian data analysis: (1) the usual rules of conditional probability fail in the quantum realm, (2) flat or weak priors lead to terrible inferences about things we care about, (3) subjective priors are incoherent, (4) Bayesian decision picks the wrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6) for Cantorian reasons we need to check our models, but this destroys the coherence of Bayesian inference. Some of the problems of Bayesian statistics arise from people trying to do things they shouldn't be trying to do, but other holes are not so easily patched. In particular, it may be a good idea to avoid flat, weak, or conventional priors, but such advice, if followed, would go against the vast majority of Bayesian practice and requires us to confront the fundamental incoherence of Bayesian inference. This does not mean that we think Bayesian inference is a bad idea, but it does mean that there is a tension between Bayesian logic and Bayesian workflow which we believe can only be resolved by considering Bayesian logic as a tool, a way of revealing inevitable misfits and incoherences in our model assumptions, rather than as an end in itself.},
	number = {1},
	urldate = {2022-12-04},
	journal = {Journal of Physics G: Nuclear and Particle Physics},
	author = {Gelman, Andrew and Yao, Yuling},
	month = jan,
	year = {2021},
	note = {arXiv:2002.06467 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {014002},
}

@book{bernardo_bayesian_1994,
	title = {Bayesian {Theory}},
	publisher = {John Wiley \& Sons},
	author = {Bernardo, José M. and Smith, Adrian F. M.},
	year = {1994},
}

@book{jaynes_probability_2003,
	title = {Probability {Theory}: {The} {Logic} of {Science}},
	isbn = {9780521592710},
	shorttitle = {Probability {Theory}},
	abstract = {The standard rules of probability can be interpreted as uniquely valid principles in logic.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Jaynes, E. T.},
	month = apr,
	year = {2003},
	note = {Google-Books-ID: tTN4HuUNXjgC},
	keywords = {Mathematics / Applied, Mathematics / Probability \& Statistics / General, Science / Physics / General, Science / Physics / Mathematical \& Computational},
}

@misc{roy_estimation_2014,
	title = {Estimation of {Causal} {Invertible} {VARMA} {Models}},
	url = {http://arxiv.org/abs/1406.4584},
	doi = {10.48550/arXiv.1406.4584},
	abstract = {We present a re-parameterization of vector autoregressive moving average (VARMA) models that allows estimation of parameters under the constraints of causality and invertibility. The parameter constraints associated with a causal invertible VARMA model are highly complex. Currently there are no procedures that can maintain the constraints in the estimated VARMA process, except in the special case of a vector autoregression (VAR), where some moment based causal estimators are available. Even in the VAR case, the available likelihood based estimators are not causal. The maximum likelihood estimator based on the full likelihood that does not condition on the initial observations by definition satisfies the causal invertible constraints but optimization of the likelihood under the complex constraints is an intractable problem. The commonly used Bayesian procedure for VAR often has posterior mass outside the causal set because the priors are not constrained to the causal set of parameters. We provide an exact mathematical solution to this problem. An \$m\$-variate VARMA\$(p, q)\$ process contains \$(p+ q) m{\textasciicircum}2 + {\textbackslash}binom\{m+1\}\{2\}\$ parameters, which must be constrained to a subset of Euclidean space in order to guarantee causality and invertibility. This space is implicitly described in this paper, through the device of parameterizing the entire space of block Toeplitz matrices in terms of positive definite matrices and orthogonal matrices. The parameterization has connection to Schur- stability of polynomials and the associated Stein transformation that are often used in dynamical systems literature. As an important by-product of our investigation, we generalize a classical result in dynamical systems to provide a characterization of Schur stable matrix polynomials.},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Roy, Anindya and McElroy, Tucker S. and Linton, Peter},
	month = jun,
	year = {2014},
	note = {arXiv:1406.4584 [math, stat]},
	keywords = {62M10, 62F30, 62H12, 91B84, G.3, Mathematics - Statistics Theory},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	issn = {1935-7524},
	doi = {10.1214/17-EJS1337SI},
	abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suffered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coefficients, which can be problematic with weakly identified parameters, such as the logistic regression coefficients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of effective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a finite slab width, whereas the original horseshoe resembles the spike-and-slab with an infinitely wide slab. Numerical experiments on synthetic and real world data illustrate the benefit of both of these theoretical advances.},
	number = {2},
	urldate = {2022-12-03},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	month = jan,
	year = {2017},
	keywords = {Statistics - Methodology},
}

@inproceedings{piironen_hyperprior_2017,
  title = {On the {{Hyperprior Choice}} for the {{Global Shrinkage Parameter}} in the {{Horseshoe Prior}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Piironen, Juho and Vehtari, Aki},
  year = {2017},
  month = apr,
  pages = {905--913},
  publisher = {PMLR},
  issn = {2640-3498},
  langid = {english}
}

@inproceedings{carvalho_handling_2009,
	title = {Handling {Sparsity} via the {Horseshoe}},
	abstract = {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.},
	language = {en},
	urldate = {2022-12-03},
	booktitle = {Proceedings of the {Twelth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	month = apr,
	year = {2009},
	pages = {73--80},
}

@article{kendall_study_1954,
	title = {A {Study} in the {Analysis} of {Stationary} {Time}-{Series}.},
	volume = {117},
	issn = {00359238},
	doi = {10.2307/2342687},
	number = {4},
	urldate = {2022-12-03},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Kendall, Maurice and Wold, Herman},
	year = {1954},
	pages = {484},
}

@article{chan_large_2016,
	title = {Large {Bayesian} {VARMAs}},
	volume = {192},
	issn = {03044076},
	doi = {10.1016/j.jeconom.2016.02.005},
	language = {en},
	number = {2},
	urldate = {2022-12-03},
	journal = {Journal of Econometrics},
	author = {Chan, Joshua C.C. and Eisenstat, Eric and Koop, Gary},
	month = jun,
	year = {2016},
	pages = {374--390},
}

@article{harvey_trends_1985,
	title = {Trends and {Cycles} in {Macroeconomic} {Time} {Series}},
	volume = {3},
	issn = {0735-0015, 1537-2707},
	doi = {10.1080/07350015.1985.10509453},
	language = {en},
	number = {3},
	urldate = {2022-12-03},
	journal = {Journal of Business \& Economic Statistics},
	author = {Harvey, A. C.},
	month = jul,
	year = {1985},
	pages = {216--227},
}

@article{zhang_stochastic_2018,
	title = {Stochastic {Volatility} {Models} with {Arma} {Innovations} an {Application} to {G7} {Inflation} {Forecasts}},
	issn = {1556-5068},
	doi = {10.2139/ssrn.3222423},
	language = {en},
	urldate = {2022-12-03},
	journal = {SSRN Electronic Journal},
	author = {Zhang, Bo and Chan, Joshua C. C. and Cross, Jamie},
	year = {2018},
}

@article{polson_half-cauchy_2012,
	title = {On the {Half}-{Cauchy} {Prior} for a {Global} {Scale} {Parameter}},
	volume = {7},
	issn = {1936-0975},
	doi = {10.1214/12-BA730},
	number = {4},
	urldate = {2022-12-03},
	journal = {Bayesian Analysis},
	author = {Polson, Nicholas G. and Scott, James G.},
	month = dec,
	year = {2012},
}

@article{bhadra_lasso_2019,
	title = {Lasso {Meets} {Horseshoe}: {A} {Survey}},
	volume = {34},
	issn = {0883-4237},
	shorttitle = {Lasso {Meets} {Horseshoe}},
	doi = {10.1214/19-STS700},
	number = {3},
	urldate = {2022-12-03},
	journal = {Statistical Science},
	author = {Bhadra, Anindya and Datta, Jyotishka and Polson, Nicholas G. and Willard, Brandon},
	month = aug,
	year = {2019},
}

@misc{catalina_latent_2021,
	title = {Latent space projection predictive inference},
	url = {http://arxiv.org/abs/2109.04702},
	doi = {10.48550/arXiv.2109.04702},
	abstract = {Given a reference model that includes all the available variables, projection predictive inference replaces its posterior with a constrained projection including only a subset of all variables. We extend projection predictive inference to enable computationally efficient variable and structure selection in models outside the exponential family. By adopting a latent space projection predictive perspective we are able to: 1) propose a unified and general framework to do variable selection in complex models while fully honouring the original model structure, 2) properly identify relevant structure and retain posterior uncertainties from the original model, and 3) provide an improved approach also for non-Gaussian models in the exponential family. We demonstrate the superior performance of our approach by thoroughly testing and comparing it against popular variable selection approaches in a wide range of settings, including realistic data sets. Our results show that our approach successfully recovers relevant terms and model structure in complex models, selecting less variables than competing approaches for realistic datasets.},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Catalina, Alejandro and Bürkner, Paul and Vehtari, Aki},
	month = sep,
	year = {2021},
	note = {arXiv:2109.04702 [stat]},
	keywords = {Statistics - Computation},
}

@article{carpenter_stan_2017,
	title = {\textit{{Stan}}: {A} {Probabilistic} {Programming} {Language}},
	volume = {76},
	issn = {1548-7660},
	shorttitle = {\textit{{Stan}}},
	doi = {10.18637/jss.v076.i01},
	language = {en},
	number = {1},
	urldate = {2022-12-03},
	journal = {Journal of Statistical Software},
	author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year = {2017},
}

@book{brooks_handbook_2011,
	edition = {0},
	title = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	isbn = {9780429138508},
	language = {en},
	urldate = {2022-12-03},
	publisher = {Chapman and Hall/CRC},
	editor = {Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	month = may,
	year = {2011},
	doi = {10.1201/b10905},
}

@article{chib_bayes_1994,
	title = {Bayes inference in regression models with {ARMA} (p, q) errors},
	volume = {64},
	issn = {03044076},
	doi = {10.1016/0304-4076(94)90063-9},
	language = {en},
	number = {1-2},
	urldate = {2022-12-03},
	journal = {Journal of Econometrics},
	author = {Chib, Siddhartha and Greenberg, Edward},
	month = sep,
	year = {1994},
	pages = {183--206},
}

@article{mogliani_bayesian_2019,
	title = {Bayesian {MIDAS} {Penalized} {Regressions}: {Estimation}, {Selection}, and {Prediction}},
	issn = {1556-5068},
	shorttitle = {Bayesian {MIDAS} {Penalized} {Regressions}},
	doi = {10.2139/ssrn.3354565},
	language = {en},
	urldate = {2022-12-03},
	journal = {SSRN Electronic Journal},
	author = {Mogliani, Matteo},
	year = {2019},
}

@article{granger_time_1976,
	title = {Time {Series} {Modelling} and {Interpretation}},
	volume = {139},
	issn = {00359238},
	doi = {10.2307/2345178},
	number = {2},
	urldate = {2022-12-03},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Granger, C. W. J. and Morris, M. J.},
	year = {1976},
	pages = {246},
}

@incollection{fomby_relationship_2013,
	title = {The {Relationship} {Between} {DSGE} and {VAR} {Models}},
	volume = {32},
	isbn = {9781781907528 9781781907535},
	language = {en},
	urldate = {2022-12-03},
	booktitle = {{VAR} {Models} in {Macroeconomics} – {New} {Developments} and {Applications}: {Essays} in {Honor} of {Christopher} {A}. {Sims}},
	publisher = {Emerald Group Publishing Limited},
	author = {Giacomini, Raffaella},
	editor = {Fomby, Thomas B. and Kilian, Lutz and Murphy, Anthony},
	month = dec,
	year = {2013},
	doi = {10.1108/S0731-9053(2013)0000031001},
	pages = {1--25},
}

@techreport{stock_why_2006,
	address = {Cambridge, MA},
	title = {Why {Has} {U}.{S}. {Inflation} {Become} {Harder} to {Forecast}?},
	language = {en},
	number = {w12324},
	urldate = {2022-12-03},
	institution = {National Bureau of Economic Research},
	author = {Stock, James and Watson, Mark},
	month = jun,
	year = {2006},
	doi = {10.3386/w12324},
	pages = {w12324},
}

@article{ives_analysis_2010,
	title = {Analysis of ecological time series with {ARMA}( \textit{p} , \textit{q} ) models},
	volume = {91},
	issn = {0012-9658},
	doi = {10.1890/09-0442.1},
	language = {en},
	number = {3},
	urldate = {2022-12-03},
	journal = {Ecology},
	author = {Ives, Anthony R. and Abbott, Karen C. and Ziebarth, Nicolas L.},
	month = mar,
	year = {2010},
	pages = {858--871},
}

@article{casella_penalized_2010,
	title = {Penalized regression, standard errors, and {Bayesian} lassos},
	volume = {5},
	issn = {1936-0975},
	doi = {10.1214/10-BA607},
	number = {2},
	urldate = {2022-12-03},
	journal = {Bayesian Analysis},
	author = {Casella, George and Ghosh, Malay and Gill, Jeff and Kyung, Minjung},
	month = jun,
	year = {2010},
}

@article{schotman_bayesian_1991,
	title = {On {Bayesian} routes to unit roots},
	volume = {6},
	issn = {08837252, 10991255},
	doi = {10.1002/jae.3950060407},
	language = {en},
	number = {4},
	urldate = {2022-12-03},
	journal = {Journal of Applied Econometrics},
	author = {Schotman, Peter C. and Van Dijk, Herman K.},
	month = oct,
	year = {1991},
	pages = {387--401},
}

@misc{sivula_uncertainty_2022,
	title = {Uncertainty in {Bayesian} {Leave}-{One}-{Out} {Cross}-{Validation} {Based} {Model} {Comparison}},
	url = {http://arxiv.org/abs/2008.10296},
	doi = {10.48550/arXiv.2008.10296},
	abstract = {Leave-one-out cross-validation (LOO-CV) is a popular method for comparing Bayesian models based on their estimated predictive performance on new, unseen, data. As leave-one-out cross-validation is based on finite observed data, there is uncertainty about the expected predictive performance on new data. By modeling this uncertainty when comparing two models, we can compute the probability that one model has a better predictive performance than the other. Modeling this uncertainty well is not trivial, and for example, it is known that the commonly used standard error estimate is often too small. We study the properties of the Bayesian LOO-CV estimator and the related uncertainty estimates when comparing two models. We provide new results of the properties both theoretically in the linear regression case and empirically for multiple different models and discuss the challenges of modeling the uncertainty. We show that problematic cases include: comparing models with similar predictions, misspecified models, and small data. In these cases, there is a weak connection in the skewness of the individual leave-one-out terms and the distribution of the error of the Bayesian LOO-CV estimator. We show that it is possible that the problematic skewness of the error distribution, which occurs when the models make similar predictions, does not fade away when the data size grows to infinity in certain situations. Based on the results, we also provide practical recommendations for the users of Bayesian LOO-CV for model comparison.},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Sivula, Tuomas and Magnusson, Måns and Matamoros, Asael Alonzo and Vehtari, Aki},
	month = mar,
	year = {2022},
	note = {arXiv:2008.10296 [stat]},
	keywords = {Statistics - Methodology},
}

@article{mitchell_bayesian_1988,
	title = {Bayesian {Variable} {Selection} in {Linear} {Regression}},
	volume = {83},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478694},
	doi = {10.1080/01621459.1988.10478694},
	language = {en},
	number = {404},
	urldate = {2022-12-03},
	journal = {Journal of the American Statistical Association},
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	month = dec,
	year = {1988},
	pages = {1023--1032},
}

@article{piironen_sparsity_2017-1,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	issn = {1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-11/issue-2/Sparsity-information-and-regularization-in-the-horseshoe-and-other-shrinkage/10.1214/17-EJS1337SI.full},
	doi = {10.1214/17-EJS1337SI},
	number = {2},
	urldate = {2022-12-03},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	month = jan,
	year = {2017},
}

@article{chan_minnesota-type_2019,
	title = {Minnesota-{Type} {Adaptive} {Hierarchical} {Priors} for {Large} {Bayesian} {VARs}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3440949},
	doi = {10.2139/ssrn.3440949},
	language = {en},
	urldate = {2022-12-03},
	journal = {SSRN Electronic Journal},
	author = {Chan, Joshua C. C.},
	year = {2019},
}

@article{kullback_information_1951,
	title = {On {Information} and {Sufficiency}},
	volume = {22},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177729694},
	doi = {10.1214/aoms/1177729694},
	language = {en},
	number = {1},
	urldate = {2022-12-03},
	journal = {The Annals of Mathematical Statistics},
	author = {Kullback, S. and Leibler, R. A.},
	month = mar,
	year = {1951},
	pages = {79--86},
}

@misc{hoffman_no-u-turn_2011,
	title = {The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}},
	shorttitle = {The {No}-{U}-{Turn} {Sampler}},
	url = {http://arxiv.org/abs/1111.4246},
	doi = {10.48550/arXiv.1111.4246},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{{\textbackslash}epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{{\textbackslash}epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Hoffman, Matthew D. and Gelman, Andrew},
	month = nov,
	year = {2011},
	note = {arXiv:1111.4246 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation},
}

@book{mcquarrie_regression_1998,
	title = {Regression and {Time} {Series} {Model} {Selection}},
	isbn = {9789810232429 9789812385451},
	url = {https://www.worldscientific.com/worldscibooks/10.1142/3573},
	language = {en},
	urldate = {2022-12-03},
	publisher = {WORLD SCIENTIFIC},
	author = {McQuarrie, Allan D R and Tsai, Chih-Ling},
	month = may,
	year = {1998},
	doi = {10.1142/3573},
}

@incollection{mills_box_2013,
	address = {London},
	title = {Box and {Jenkins}: {Time} {Series} {Analysis}, {Forecasting} and {Control}},
	isbn = {9781349350278 9781137291264},
	shorttitle = {Box and {Jenkins}},
	url = {http://link.springer.com/10.1057/9781137291264_6},
	language = {en},
	urldate = {2022-12-03},
	booktitle = {A {Very} {British} {Affair}},
	publisher = {Palgrave Macmillan UK},
	author = {Box, George},
	collaborator = {Mills, Terence C.},
	year = {2013},
	doi = {10.1057/9781137291264_6},
	pages = {161--215},
}

@article{nardi_autoregressive_2011,
	title = {Autoregressive process modeling via the {Lasso} procedure},
	volume = {102},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X10002186},
	doi = {10.1016/j.jmva.2010.10.012},
	language = {en},
	number = {3},
	urldate = {2022-12-03},
	journal = {Journal of Multivariate Analysis},
	author = {Nardi, Y. and Rinaldo, A.},
	month = mar,
	year = {2011},
	pages = {528--549},
}

@article{chan_subset_2011,
	title = {Subset {ARMA} selection via the adaptive {Lasso}},
	volume = {4},
	issn = {19387989, 19387997},
	url = {http://www.intlpress.com/site/pub/pages/journals/items/sii/content/vols/0004/0002/a014/},
	doi = {10.4310/SII.2011.v4.n2.a14},
	language = {en},
	number = {2},
	urldate = {2022-12-03},
	journal = {Statistics and Its Interface},
	author = {Chan, Kung-Sik and Chen, Kun},
	year = {2011},
	pages = {197--205},
}

@article{sims_understanding_1991,
	title = {Understanding {Unit} {Rooters}: {A} {Helicopter} {Tour}},
	volume = {59},
	issn = {00129682},
	shorttitle = {Understanding {Unit} {Rooters}},
	url = {https://www.jstor.org/stable/2938280?origin=crossref},
	doi = {10.2307/2938280},
	number = {6},
	urldate = {2022-12-03},
	journal = {Econometrica},
	author = {Sims, Christopher A. and Uhlig, Harald},
	month = nov,
	year = {1991},
	pages = {1591},
}

@article{sims_bayesian_1988,
	title = {Bayesian skepticism on unit root econometrics},
	volume = {12},
	issn = {01651889},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0165188988900504},
	doi = {10.1016/0165-1889(88)90050-4},
	language = {en},
	number = {2-3},
	urldate = {2022-12-03},
	journal = {Journal of Economic Dynamics and Control},
	author = {Sims, Christopher A.},
	month = jun,
	year = {1988},
	pages = {463--474},
}

@article{chan_moving_2013,
	title = {Moving average stochastic volatility models with application to inflation forecast},
	volume = {176},
	issn = {03044076},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304407613001255},
	doi = {10.1016/j.jeconom.2013.05.003},
	language = {en},
	number = {2},
	urldate = {2022-12-03},
	journal = {Journal of Econometrics},
	author = {Chan, Joshua C.C.},
	month = oct,
	year = {2013},
	pages = {162--172},
}

@techreport{giannone_prior_2012,
	address = {Cambridge, MA},
	title = {Prior {Selection} for {Vector} {Autoregressions}},
	url = {http://www.nber.org/papers/w18467.pdf},
	language = {en},
	number = {w18467},
	urldate = {2022-12-03},
	institution = {National Bureau of Economic Research},
	author = {Giannone, Domenico and Lenza, Michele and Primiceri, Giorgio},
	month = oct,
	year = {2012},
	doi = {10.3386/w18467},
	pages = {w18467},
}

@article{schwarz_estimating_1978,
	title = {Estimating the {Dimension} of a {Model}},
	volume = {6},
	issn = {0090-5364},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full},
	doi = {10.1214/aos/1176344136},
	number = {2},
	urldate = {2022-12-03},
	journal = {The Annals of Statistics},
	author = {Schwarz, Gideon},
	month = mar,
	year = {1978},
}

@article{kohns_nowcasting_2022,
	title = {Nowcasting growth using {Google} {Trends} data: {A} {Bayesian} {Structural} {Time} {Series} model},
	issn = {0169-2070},
	shorttitle = {Nowcasting growth using {Google} {Trends} data},
	doi = {10.1016/j.ijforecast.2022.05.002},
	abstract = {This paper investigates the benefits of internet search data in the form of Google Trends for nowcasting real U.S. GDP growth in real time through the lens of mixed frequency Bayesian Structural Time Series (BSTS) models. We augment and enhance both model and methodology to make these better amenable to nowcasting with large number of potential covariates. Specifically, we allow shrinking state variances towards zero to avoid overfitting, extend the SSVS (spike and slab variable selection) prior to the more flexible normal-inverse-gamma prior which stays agnostic about the underlying model size, as well as adapt the horseshoe prior to the BSTS. The application to nowcasting GDP growth as well as a simulation study demonstrate that the horseshoe prior BSTS improves markedly upon the SSVS and the original BSTS model with the largest gains in dense data-generating-processes. Our application also shows that a large dimensional set of search terms is able to improve nowcasts early in a specific quarter before other macroeconomic data become available. Search terms with high inclusion probability have good economic interpretation, reflecting leading signals of economic anxiety and wealth effects.},
	language = {en},
	urldate = {2022-10-21},
	journal = {International Journal of Forecasting},
	author = {Kohns, David and Bhattacharjee, Arnab},
	month = aug,
	year = {2022},
	keywords = {Global-local priors, Google Trends, Non-centred state space, Nowcasting, Shrinkage},
}

@misc{mclatchie_bayesian_2022,
	title = {Bayesian order identification of {ARMA} models with projection predictive inference},
	url = {http://arxiv.org/abs/2208.14824},
	doi = {10.48550/arXiv.2208.14824},
	abstract = {Auto-regressive moving-average (ARMA) models are ubiquitous forecasting tools. Parsimony in such models is highly valued for their interpretability and computational tractability, and as such the identification of model orders remains a fundamental task. We propose a novel method of ARMA order identification through projection predictive inference, which is grounded in Bayesian decision theory and naturally allows for uncertainty communication. It benefits from improved stability through the use of a reference model. The procedure consists of two steps: in the first, the practitioner incorporates their understanding of underlying data-generating process into a reference model, which we latterly project onto possibly parsimonious submodels. These submodels are optimally inferred to best replicate the predictive performance of the reference model. We further propose a search heuristic amenable to the ARMA framework. We show that the submodels selected by our procedure exhibit predictive performance at least as good as those produced by auto.arima over simulated and real-data experiments, and in some cases out-perform the latter. Finally we show that our procedure is robust to noise, and scales well to larger data.},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {McLatchie, Yann and Matamoros, Asael Alonzo and Kohns, David and Vehtari, Aki},
	month = nov,
	year = {2022},
	note = {arXiv:2208.14824 [stat]},
	keywords = {Statistics - Computation, Statistics - Methodology},
}

@article{vehtari_practical_2017,
	title = {Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}},
	volume = {27},
	issn = {1573-1375},
	doi = {10.1007/s11222-016-9696-4},
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
	language = {en},
	number = {5},
	urldate = {2020-07-01},
	journal = {Statistics and Computing},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	month = sep,
	year = {2017},
	pages = {1413--1432},
}

@misc{xu_bayesian_2022,
	title = {Bayesian {Inference} with the l1-ball {Prior}: {Solving} {Combinatorial} {Problems} with {Exact} {Zeros}},
	shorttitle = {Bayesian {Inference} with the l1-ball {Prior}},
	url = {http://arxiv.org/abs/2006.01340},
	doi = {10.48550/arXiv.2006.01340},
	abstract = {The l1-regularization is very popular in high dimensional statistics -- it changes a combinatorial problem of choosing which subset of the parameter are zero, into a simple continuous optimization. Using a continuous prior concentrated near zero, the Bayesian counterparts are successful in quantifying the uncertainty in the variable selection problems; nevertheless, the lack of exact zeros makes it difficult for broader problems such as the change-point detection and rank selection. Inspired by the duality of the l1-regularization as a constraint onto an l1-ball, we propose a new prior by projecting a continuous distribution onto the l1-ball. This creates a positive probability on the ball boundary, which contains both continuous elements and exact zeros. Unlike the spike-and-slab prior, this l1-ball projection is continuous and differentiable almost surely, making the posterior estimation amenable to the Hamiltonian Monte Carlo algorithm. We examine the properties, such as the volume change due to the projection, the connection to the combinatorial prior, the minimax concentration rate in the linear problem. We demonstrate the usefulness of exact zeros that simplify the combinatorial problems, such as the change-point detection in time series, the dimension selection of mixture model and the low-rank-plus-sparse change detection in the medical images.},
	urldate = {2022-12-03},
	publisher = {arXiv},
	author = {Xu, Maoran and Duan, Leo L.},
	month = apr,
	year = {2022},
	note = {arXiv:2006.01340 [stat]},
	keywords = {Statistics - Methodology},
}

@article{zhang_bayesian_2022,
	title = {Bayesian {Regression} {Using} a {Prior} on the {Model} {Fit}: {The} {R2}-{D2} {Shrinkage} {Prior}},
	volume = {117},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Bayesian {Regression} {Using} a {Prior} on the {Model} {Fit}},
	doi = {10.1080/01621459.2020.1825449},
	language = {en},
	number = {538},
	urldate = {2022-06-09},
	journal = {Journal of the American Statistical Association},
	author = {Zhang, Yan Dora and Naughton, Brian P. and Bondell, Howard D. and Reich, Brian J.},
	month = apr,
	year = {2022},
	pages = {862--874},
	file = {Submitted Version:/Users/yannmclatchie/Zotero/storage/N3Z6TMZJ/Zhang et al. - 2022 - Bayesian Regression Using a Prior on the Model Fit.pdf:application/pdf},
}

@article{heaps_enforcing_2022,
	title = {Enforcing stationarity through the prior in vector autoregressions},
	issn = {1061-8600, 1537-2715},
	doi = {10.1080/10618600.2022.2079648},
	abstract = {Stationarity is a very common assumption in time series analysis. A vector autoregressive process is stationary if and only if the roots of its characteristic equation lie outside the unit circle, constraining the autoregressive coefficient matrices to lie in the stationary region. However, the stationary region has a highly complex geometry which impedes specification of a prior distribution. In this work, an unconstrained reparameterization of a stationary vector autoregression is presented. The new parameters are partial autocorrelation matrices, which are interpretable, and can be transformed bijectively to the space of unconstrained square matrices through a simple mapping of their singular values. This transformation preserves various structural forms of the partial autocorrelation matrices and readily facilitates specification of a prior. Properties of this prior are described along with an important special case which is exchangeable with respect to the order of the elements in the observation vector. Posterior inference and computation are described and implemented using Hamiltonian Monte Carlo via Stan. The prior and inferential procedures are illustrated with an application to a macroeconomic time series which highlights the benefits of enforcing stationarity and encouraging shrinkage towards a sensible parametric structure. Supplementary materials for this article are available in the ancillary files section.},
	urldate = {2022-12-03},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Heaps, Sarah E.},
	month = jun,
	year = {2022},
	keywords = {Statistics - Methodology},
	pages = {1--10},
}

@unpublished{duchi_elastic_2009,
	title = {Elastic net projections},
	url = {https://web.stanford.edu/~jduchi/projects/proj_elastic_net.pdf},
	author = {Duchi, John},
	month = dec,
	year = {2009},
}

@article{gelman_prior_2017,
	title = {The {Prior} {Can} {Often} {Only} {Be} {Understood} in the {Context} of the {Likelihood}},
	volume = {19},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/19/10/555},
	doi = {10.3390/e19100555},
	language = {en},
	number = {10},
	urldate = {2020-08-01},
	journal = {Entropy},
	author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
	month = oct,
	year = {2017},
	pages = {555},
	file = {Full Text:/Users/yannmclatchie/Zotero/storage/8R5MQ4E7/Gelman et al. - 2017 - The Prior Can Often Only Be Understood in the Cont.pdf:application/pdf},
}


@article{aguilar_intuitive_2023,
	title = {Intuitive joint priors for {Bayesian} linear multilevel models: {The} {R2D2M2} prior},
	volume = {17},
	issn = {1935-7524},
	shorttitle = {Intuitive joint priors for {Bayesian} linear multilevel models},
	doi = {10.1214/23-EJS2136},
	number = {1},
	urldate = {2023-07-02},
	journal = {Electronic Journal of Statistics},
	author = {Aguilar, Javier Enrique and Bürkner, Paul-Christian},
	month = jan,
	year = {2023},
}

@article{Strumbelj+etal:2024:software,
title={Past, present, and future of software for {Bayesian} inference},
author={Erik {\v S}trumbelj and Alexandre Bouchard-C\^{o}té and Jukka Corander and Andrew Gelman and H\o{a}vard Rue and Lawrence Murray and Henri Pesonen and Martyn Plummer and Aki Vehtari},
journal={Statistical Science},
year=2024,
volume=39,
number=1,
pages={46--61}
}

